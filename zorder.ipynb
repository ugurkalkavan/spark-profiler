{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7464de6a-2924-495d-afd4-0612d3cf8169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T11:42:21.982569Z",
     "iopub.status.busy": "2024-06-03T11:42:21.982059Z",
     "iopub.status.idle": "2024-06-03T11:42:22.025205Z",
     "shell.execute_reply": "2024-06-03T11:42:22.024665Z",
     "shell.execute_reply.started": "2024-06-03T11:42:21.982523Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ZOrderExample').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73c66765-1e78-4a0b-b423-4070996368e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T11:47:09.421387Z",
     "iopub.status.busy": "2024-06-03T11:47:09.421102Z",
     "iopub.status.idle": "2024-06-03T11:47:09.425143Z",
     "shell.execute_reply": "2024-06-03T11:47:09.424631Z",
     "shell.execute_reply.started": "2024-06-03T11:47:09.421369Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, shiftLeft, lit\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "\n",
    "def compute_z_order(df, cols):\n",
    "    df = df.withColumn(\"z_order\", lit(0))\n",
    "\n",
    "    for i, col_name in enumerate(cols):\n",
    "        df = df.withColumn(\"z_order\", shiftLeft(df.z_order, 1) | df[col_name].cast(LongType()))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa77ce00-efb3-467a-a18b-dbc3f5c65ad0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T11:47:02.615632Z",
     "iopub.status.busy": "2024-06-03T11:47:02.614932Z",
     "iopub.status.idle": "2024-06-03T11:47:02.885288Z",
     "shell.execute_reply": "2024-06-03T11:47:02.884768Z",
     "shell.execute_reply.started": "2024-06-03T11:47:02.615583Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(shiftleft(z_order, 1) OR CAST(col1 AS BIGINT))\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"INT\" and \"BIGINT\").;\n'Project [Name#47, col1#48L, col2#49L, col3#50L, (shiftleft(z_order#55, 1) OR cast(col1#48L as bigint)) AS z_order#61]\n+- Project [Name#47, col1#48L, col2#49L, col3#50L, 0 AS z_order#55]\n   +- LogicalRDD [Name#47, col1#48L, col2#49L, col3#50L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol3\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Using z-order function\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m compute_z_order(df, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol3\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[19], line 10\u001b[0m, in \u001b[0;36mcompute_z_order\u001b[0;34m(df, cols)\u001b[0m\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz_order\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, col_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cols):\n\u001b[0;32m---> 10\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz_order\u001b[39m\u001b[38;5;124m\"\u001b[39m, shiftLeft(df\u001b[38;5;241m.\u001b[39mz_order, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m|\u001b[39m df[col_name]\u001b[38;5;241m.\u001b[39mcast(LongType()))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/dataframe.py:5174\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5171\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5172\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5173\u001b[0m     )\n\u001b[0;32m-> 5174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(shiftleft(z_order, 1) OR CAST(col1 AS BIGINT))\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"INT\" and \"BIGINT\").;\n'Project [Name#47, col1#48L, col2#49L, col3#50L, (shiftleft(z_order#55, 1) OR cast(col1#48L as bigint)) AS z_order#61]\n+- Project [Name#47, col1#48L, col2#49L, col3#50L, 0 AS z_order#55]\n   +- LogicalRDD [Name#47, col1#48L, col2#49L, col3#50L], false\n"
     ]
    }
   ],
   "source": [
    "# Here is an example DataFrame:\n",
    "data = [(\"Alice\", 1,2,3), (\"Bob\", 4,5,6), (\"Charlie\", 7,8,9)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# Using z-order function\n",
    "df = compute_z_order(df, [\"col1\", \"col2\", \"col3\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc221c-870a-462a-bed4-91222c684bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
