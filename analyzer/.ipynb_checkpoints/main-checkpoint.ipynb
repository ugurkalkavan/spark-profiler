{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8887d531-34ed-4357-8fb9-9eb9cf492b3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T22:37:27.253702Z",
     "iopub.status.busy": "2024-06-06T22:37:27.253638Z",
     "iopub.status.idle": "2024-06-06T22:37:27.278984Z",
     "shell.execute_reply": "2024-06-06T22:37:27.278656Z",
     "shell.execute_reply.started": "2024-06-06T22:37:27.253695Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b4f51c-f745-4877-b965-d3e160077f03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T22:37:27.279448Z",
     "iopub.status.busy": "2024-06-06T22:37:27.279375Z",
     "iopub.status.idle": "2024-06-06T22:37:27.281364Z",
     "shell.execute_reply": "2024-06-06T22:37:27.281038Z",
     "shell.execute_reply.started": "2024-06-06T22:37:27.279441Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SPARK_API_ENDPOINT = \"http://localhost:4040\"\n",
    "spark_app_id = \"local-1717710582737\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5012433-4b2a-4bc0-90e8-89b32d4e17c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T22:37:27.281791Z",
     "iopub.status.busy": "2024-06-06T22:37:27.281724Z",
     "iopub.status.idle": "2024-06-06T22:37:27.284064Z",
     "shell.execute_reply": "2024-06-06T22:37:27.283827Z",
     "shell.execute_reply.started": "2024-06-06T22:37:27.281784Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_api = SparkRestAPI(base_url = SPARK_API_ENDPOINT, app_id = spark_app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "911a52ca-d2f5-4e99-abdc-1b8a799385db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T22:37:36.698544Z",
     "iopub.status.busy": "2024-06-06T22:37:36.697753Z",
     "iopub.status.idle": "2024-06-06T22:37:37.422822Z",
     "shell.execute_reply": "2024-06-06T22:37:37.422528Z",
     "shell.execute_reply.started": "2024-06-06T22:37:36.698486Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql = spark_api.get_sql()\n",
    "stage = spark_api.get_stage()\n",
    "job = spark_api.get_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4964632-e2d0-4bbb-9ee2-f29b9ef57766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T22:37:40.859577Z",
     "iopub.status.busy": "2024-06-06T22:37:40.858879Z",
     "iopub.status.idle": "2024-06-06T22:37:40.883559Z",
     "shell.execute_reply": "2024-06-06T22:37:40.883124Z",
     "shell.execute_reply.started": "2024-06-06T22:37:40.859528Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'status': 'COMPLETED',\n",
       "  'description': 'spill',\n",
       "  'planDescription': '== Physical Plan ==\\nAdaptiveSparkPlan (13)\\n+- == Final Plan ==\\n   * HashAggregate (9)\\n   +- ShuffleQueryStage (8), Statistics(sizeInBytes=160.0 B, rowCount=10)\\n      +- Exchange (7)\\n         +- * HashAggregate (6)\\n            +- TableCacheQueryStage (5), Statistics(sizeInBytes=38.1 MiB, rowCount=1.00E+7)\\n               +- InMemoryTableScan (1)\\n                     +- InMemoryRelation (2)\\n                           +- * Project (4)\\n                              +- * Scan ExistingRDD (3)\\n+- == Initial Plan ==\\n   HashAggregate (12)\\n   +- Exchange (11)\\n      +- HashAggregate (10)\\n         +- InMemoryTableScan (1)\\n               +- InMemoryRelation (2)\\n                     +- * Project (4)\\n                        +- * Scan ExistingRDD (3)\\n\\n\\n(1) InMemoryTableScan\\nOutput: []\\n\\n(2) InMemoryRelation\\nArguments: [id#2], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@4214a019,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) Project [value#0 AS id#2]\\n+- *(1) Scan ExistingRDD[value#0]\\n,None)\\n\\n(3) Scan ExistingRDD [codegen id : 1]\\nOutput [1]: [value#0]\\nArguments: [value#0], MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\\n\\n(4) Project [codegen id : 1]\\nOutput [1]: [value#0 AS id#2]\\nInput [1]: [value#0]\\n\\n(5) TableCacheQueryStage\\nOutput: []\\nArguments: 0\\n\\n(6) HashAggregate [codegen id : 1]\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#28L]\\nResults [1]: [count#29L]\\n\\n(7) Exchange\\nInput [1]: [count#29L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=46]\\n\\n(8) ShuffleQueryStage\\nOutput [1]: [count#29L]\\nArguments: 1\\n\\n(9) HashAggregate [codegen id : 2]\\nInput [1]: [count#29L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#10L]\\nResults [1]: [count(1)#10L AS count#11L]\\n\\n(10) HashAggregate\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#28L]\\nResults [1]: [count#29L]\\n\\n(11) Exchange\\nInput [1]: [count#29L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=19]\\n\\n(12) HashAggregate\\nInput [1]: [count#29L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#10L]\\nResults [1]: [count(1)#10L AS count#11L]\\n\\n(13) AdaptiveSparkPlan\\nOutput [1]: [count#11L]\\nArguments: isFinalPlan=true\\n\\n',\n",
       "  'submissionTime': '2024-06-06T21:49:57.849GMT',\n",
       "  'duration': 7943,\n",
       "  'runningJobIds': [],\n",
       "  'successJobIds': [0, 1, 2],\n",
       "  'failedJobIds': [],\n",
       "  'nodes': [{'nodeId': 9,\n",
       "    'nodeName': 'Scan ExistingRDD',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': [{'name': 'number of output rows', 'value': '10,000,000'}]},\n",
       "   {'nodeId': 8,\n",
       "    'nodeName': 'Project',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': []},\n",
       "   {'nodeId': 7,\n",
       "    'nodeName': 'WholeStageCodegen (1)',\n",
       "    'metrics': [{'name': 'duration',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n32.5 s (0 ms, 3.2 s, 3.5 s (stage 0.0: task 9))'}]},\n",
       "   {'nodeId': 6,\n",
       "    'nodeName': 'InMemoryTableScan',\n",
       "    'metrics': [{'name': 'number of output rows', 'value': '10,000,000'}]},\n",
       "   {'nodeId': 5,\n",
       "    'nodeName': 'HashAggregate',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': [{'name': 'time in aggregation build',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n665 ms (53 ms, 68 ms, 70 ms (stage 1.0: task 16))'},\n",
       "     {'name': 'number of output rows', 'value': '10'}]},\n",
       "   {'nodeId': 4,\n",
       "    'nodeName': 'WholeStageCodegen (1)',\n",
       "    'metrics': [{'name': 'duration',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n871 ms (87 ms, 87 ms, 88 ms (stage 1.0: task 17))'}]},\n",
       "   {'nodeId': 3,\n",
       "    'nodeName': 'Exchange',\n",
       "    'metrics': [{'name': 'shuffle records written', 'value': '10'},\n",
       "     {'name': 'local merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'shuffle write time',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n104 ms (0 ms, 9 ms, 14 ms (stage 1.0: task 13))'},\n",
       "     {'name': 'remote merged bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'local merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'corrupt merged block chunks', 'value': '0'},\n",
       "     {'name': 'remote merged reqs duration', 'value': '0 ms'},\n",
       "     {'name': 'remote merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'records read', 'value': '10'},\n",
       "     {'name': 'local bytes read', 'value': '581.0 B'},\n",
       "     {'name': 'fetch wait time', 'value': '0 ms'},\n",
       "     {'name': 'remote bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'merged fetch fallback count', 'value': '0'},\n",
       "     {'name': 'local blocks read', 'value': '10'},\n",
       "     {'name': 'remote merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'remote blocks read', 'value': '0'},\n",
       "     {'name': 'data size',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n160.0 B (0.0 B, 16.0 B, 16.0 B (stage 1.0: task 12))'},\n",
       "     {'name': 'local merged bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'number of partitions', 'value': '1'},\n",
       "     {'name': 'remote reqs duration', 'value': '0 ms'},\n",
       "     {'name': 'remote bytes read to disk', 'value': '0.0 B'},\n",
       "     {'name': 'shuffle bytes written',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n581.0 B (0.0 B, 58.0 B, 59.0 B (stage 1.0: task 19))'}]},\n",
       "   {'nodeId': 2,\n",
       "    'nodeName': 'HashAggregate',\n",
       "    'wholeStageCodegenId': 2,\n",
       "    'metrics': [{'name': 'time in aggregation build', 'value': '9 ms'},\n",
       "     {'name': 'number of output rows', 'value': '1'}]},\n",
       "   {'nodeId': 1,\n",
       "    'nodeName': 'WholeStageCodegen (2)',\n",
       "    'metrics': [{'name': 'duration', 'value': '10 ms'}]},\n",
       "   {'nodeId': 0, 'nodeName': 'AdaptiveSparkPlan', 'metrics': []}],\n",
       "  'edges': [{'fromId': 2, 'toId': 0},\n",
       "   {'fromId': 3, 'toId': 2},\n",
       "   {'fromId': 5, 'toId': 3},\n",
       "   {'fromId': 6, 'toId': 5},\n",
       "   {'fromId': 8, 'toId': 6},\n",
       "   {'fromId': 9, 'toId': 8}]},\n",
       " {'id': 1,\n",
       "  'status': 'COMPLETED',\n",
       "  'description': 'spill',\n",
       "  'planDescription': '== Physical Plan ==\\nExecute CreateViewCommand (1)\\n   +- CreateViewCommand (2)\\n         +- Project (4)\\n            +- LogicalRDD (3)\\n\\n\\n(1) Execute CreateViewCommand\\nOutput: []\\n\\n(2) CreateViewCommand\\nArguments: `records`, false, true, LocalTempView, true\\n\\n(3) LogicalRDD\\nArguments: [value#0], false\\n\\n(4) Project\\nArguments: [value#0 AS id#2]\\n\\n',\n",
       "  'submissionTime': '2024-06-06T21:50:05.849GMT',\n",
       "  'duration': 9,\n",
       "  'runningJobIds': [],\n",
       "  'successJobIds': [],\n",
       "  'failedJobIds': [],\n",
       "  'nodes': [{'nodeId': 0,\n",
       "    'nodeName': 'Execute CreateViewCommand',\n",
       "    'metrics': []}],\n",
       "  'edges': []},\n",
       " {'id': 2,\n",
       "  'status': 'COMPLETED',\n",
       "  'description': 'spill',\n",
       "  'planDescription': '== Physical Plan ==\\nAdaptiveSparkPlan (23)\\n+- == Final Plan ==\\n   * HashAggregate (19)\\n   +- ShuffleQueryStage (18), Statistics(sizeInBytes=31.3 KiB, rowCount=2.00E+3)\\n      +- Exchange (17)\\n         +- * HashAggregate (16)\\n            +- TableCacheQueryStage (15), Statistics(sizeInBytes=76.3 MiB, rowCount=2.00E+7)\\n               +- InMemoryTableScan (1)\\n                     +- InMemoryRelation (2)\\n                           +- AdaptiveSparkPlan (14)\\n                              +- Union (13)\\n                                 :- Exchange (7)\\n                                 :  +- InMemoryTableScan (3)\\n                                 :        +- InMemoryRelation (4)\\n                                 :              +- * Project (6)\\n                                 :                 +- * Scan ExistingRDD (5)\\n                                 +- Exchange (12)\\n                                    +- InMemoryTableScan (8)\\n                                          +- InMemoryRelation (9)\\n                                                +- * Project (11)\\n                                                   +- * Scan ExistingRDD (10)\\n+- == Initial Plan ==\\n   HashAggregate (22)\\n   +- Exchange (21)\\n      +- HashAggregate (20)\\n         +- InMemoryTableScan (1)\\n               +- InMemoryRelation (2)\\n                     +- AdaptiveSparkPlan (14)\\n                        +- Union (13)\\n                           :- Exchange (7)\\n                           :  +- InMemoryTableScan (3)\\n                           :        +- InMemoryRelation (4)\\n                           :              +- * Project (6)\\n                           :                 +- * Scan ExistingRDD (5)\\n                           +- Exchange (12)\\n                              +- InMemoryTableScan (8)\\n                                    +- InMemoryRelation (9)\\n                                          +- * Project (11)\\n                                             +- * Scan ExistingRDD (10)\\n\\n\\n(1) InMemoryTableScan\\nOutput: []\\n\\n(2) InMemoryRelation\\nArguments: [id#2], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@4214a019,StorageLevel(disk, memory, deserialized, 1 replicas),AdaptiveSparkPlan isFinalPlan=true\\n+- == Final Plan ==\\n   Union\\n   :- ShuffleQueryStage 2\\n   :  +- Exchange RoundRobinPartitioning(1000), REPARTITION_BY_NUM, [plan_id=111]\\n   :     +- TableCacheQueryStage 0\\n   :        +- InMemoryTableScan [id#2]\\n   :              +- InMemoryRelation [id#2], StorageLevel(disk, memory, deserialized, 1 replicas)\\n   :                    +- *(1) Project [value#0 AS id#2]\\n   :                       +- *(1) Scan ExistingRDD[value#0]\\n   +- ShuffleQueryStage 3\\n      +- Exchange RoundRobinPartitioning(1000), REPARTITION_BY_NUM, [plan_id=115]\\n         +- TableCacheQueryStage 1\\n            +- InMemoryTableScan [id#2]\\n                  +- InMemoryRelation [id#2], StorageLevel(disk, memory, deserialized, 1 replicas)\\n                        +- *(1) Project [value#0 AS id#2]\\n                           +- *(1) Scan ExistingRDD[value#0]\\n+- == Initial Plan ==\\n   Union\\n   :- Exchange RoundRobinPartitioning(1000), REPARTITION_BY_NUM, [plan_id=75]\\n   :  +- InMemoryTableScan [id#2]\\n   :        +- InMemoryRelation [id#2], StorageLevel(disk, memory, deserialized, 1 replicas)\\n   :              +- *(1) Project [value#0 AS id#2]\\n   :                 +- *(1) Scan ExistingRDD[value#0]\\n   +- Exchange RoundRobinPartitioning(1000), REPARTITION_BY_NUM, [plan_id=77]\\n      +- InMemoryTableScan [id#2]\\n            +- InMemoryRelation [id#2], StorageLevel(disk, memory, deserialized, 1 replicas)\\n                  +- *(1) Project [value#0 AS id#2]\\n                     +- *(1) Scan ExistingRDD[value#0]\\n,None)\\n\\n(3) InMemoryTableScan\\nOutput [1]: [id#2]\\nArguments: [id#2]\\n\\n(4) InMemoryRelation\\nArguments: [id#2], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@4214a019,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) Project [value#0 AS id#2]\\n+- *(1) Scan ExistingRDD[value#0]\\n,None)\\n\\n(5) Scan ExistingRDD [codegen id : 1]\\nOutput [1]: [value#0]\\nArguments: [value#0], MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\\n\\n(6) Project [codegen id : 1]\\nOutput [1]: [value#0 AS id#2]\\nInput [1]: [value#0]\\n\\n(7) Exchange\\nInput [1]: [id#2]\\nArguments: RoundRobinPartitioning(1000), REPARTITION_BY_NUM, [plan_id=75]\\n\\n(8) InMemoryTableScan\\nOutput [1]: [id#2]\\nArguments: [id#2]\\n\\n(9) InMemoryRelation\\nArguments: [id#2], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@4214a019,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) Project [value#0 AS id#2]\\n+- *(1) Scan ExistingRDD[value#0]\\n,None)\\n\\n(10) Scan ExistingRDD [codegen id : 1]\\nOutput [1]: [value#0]\\nArguments: [value#0], MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\\n\\n(11) Project [codegen id : 1]\\nOutput [1]: [value#0 AS id#2]\\nInput [1]: [value#0]\\n\\n(12) Exchange\\nInput [1]: [id#2]\\nArguments: RoundRobinPartitioning(1000), REPARTITION_BY_NUM, [plan_id=77]\\n\\n(13) Union\\n\\n(14) AdaptiveSparkPlan\\nOutput [1]: [id#2]\\nArguments: isFinalPlan=false\\n\\n(15) TableCacheQueryStage\\nOutput: []\\nArguments: 0\\n\\n(16) HashAggregate [codegen id : 1]\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#97L]\\nResults [1]: [count#98L]\\n\\n(17) Exchange\\nInput [1]: [count#98L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=172]\\n\\n(18) ShuffleQueryStage\\nOutput [1]: [count#98L]\\nArguments: 1\\n\\n(19) HashAggregate [codegen id : 2]\\nInput [1]: [count#98L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#79L]\\nResults [1]: [count(1)#79L AS count#80L]\\n\\n(20) HashAggregate\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#97L]\\nResults [1]: [count#98L]\\n\\n(21) Exchange\\nInput [1]: [count#98L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=97]\\n\\n(22) HashAggregate\\nInput [1]: [count#98L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#79L]\\nResults [1]: [count(1)#79L AS count#80L]\\n\\n(23) AdaptiveSparkPlan\\nOutput [1]: [count#80L]\\nArguments: isFinalPlan=true\\n\\n',\n",
       "  'submissionTime': '2024-06-06T21:50:06.129GMT',\n",
       "  'duration': 19244,\n",
       "  'runningJobIds': [],\n",
       "  'successJobIds': [5, 6, 7, 3, 4],\n",
       "  'failedJobIds': [],\n",
       "  'nodes': [{'nodeId': 18,\n",
       "    'nodeName': 'Scan ExistingRDD',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': [{'name': 'number of output rows', 'value': '0'}]},\n",
       "   {'nodeId': 17,\n",
       "    'nodeName': 'Project',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': []},\n",
       "   {'nodeId': 16,\n",
       "    'nodeName': 'WholeStageCodegen (1)',\n",
       "    'metrics': [{'name': 'duration',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0 ms (0 ms, 0 ms, 0 ms (stage 5.0: task 37))'}]},\n",
       "   {'nodeId': 15,\n",
       "    'nodeName': 'InMemoryTableScan',\n",
       "    'metrics': [{'name': 'number of output rows', 'value': '10,000,000'}]},\n",
       "   {'nodeId': 14,\n",
       "    'nodeName': 'Exchange',\n",
       "    'metrics': [{'name': 'shuffle records written', 'value': '10,000,000'},\n",
       "     {'name': 'local merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'shuffle write time',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n751 ms (0 ms, 0 ms, 88 ms (stage 5.0: task 39))'},\n",
       "     {'name': 'remote merged bytes read',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0.0 B (0.0 B, 0.0 B, 0.0 B (stage 11.0: task 2049))'},\n",
       "     {'name': 'local merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'corrupt merged block chunks', 'value': '0'},\n",
       "     {'name': 'remote merged reqs duration',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0 ms (0 ms, 0 ms, 0 ms (stage 11.0: task 2049))'},\n",
       "     {'name': 'remote merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'records read', 'value': '10,000,000'},\n",
       "     {'name': 'local bytes read',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n49.4 MiB (0.0 B, 0.0 B, 51.0 KiB (stage 8.0: task 1985))'},\n",
       "     {'name': 'fetch wait time',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0 ms (0 ms, 0 ms, 0 ms (stage 11.0: task 2049))'},\n",
       "     {'name': 'remote bytes read',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0.0 B (0.0 B, 0.0 B, 0.0 B (stage 11.0: task 2049))'},\n",
       "     {'name': 'merged fetch fallback count', 'value': '0'},\n",
       "     {'name': 'local blocks read', 'value': '10,000'},\n",
       "     {'name': 'remote merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'remote blocks read', 'value': '0'},\n",
       "     {'name': 'data size',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n152.6 MiB (0.0 B, 0.0 B, 15.3 MiB (stage 5.0: task 37))'},\n",
       "     {'name': 'local merged bytes read',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0.0 B (0.0 B, 0.0 B, 0.0 B (stage 11.0: task 2049))'},\n",
       "     {'name': 'number of partitions', 'value': '1,000'},\n",
       "     {'name': 'remote reqs duration',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0 ms (0 ms, 0 ms, 0 ms (stage 11.0: task 2049))'},\n",
       "     {'name': 'remote bytes read to disk',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0.0 B (0.0 B, 0.0 B, 0.0 B (stage 11.0: task 2049))'},\n",
       "     {'name': 'shuffle bytes written',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n49.4 MiB (0.0 B, 0.0 B, 5.1 MiB (stage 5.0: task 32))'}]},\n",
       "   {'nodeId': 13,\n",
       "    'nodeName': 'Scan ExistingRDD',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': [{'name': 'number of output rows', 'value': '0'}]},\n",
       "   {'nodeId': 12,\n",
       "    'nodeName': 'Project',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': []},\n",
       "   {'nodeId': 11,\n",
       "    'nodeName': 'WholeStageCodegen (1)',\n",
       "    'metrics': [{'name': 'duration',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0 ms (0 ms, 0 ms, 0 ms (stage 5.0: task 37))'}]},\n",
       "   {'nodeId': 10,\n",
       "    'nodeName': 'InMemoryTableScan',\n",
       "    'metrics': [{'name': 'number of output rows', 'value': '10,000,000'}]},\n",
       "   {'nodeId': 9,\n",
       "    'nodeName': 'Exchange',\n",
       "    'metrics': [{'name': 'shuffle records written', 'value': '10,000,000'},\n",
       "     {'name': 'local merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'shuffle write time',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n2.1 s (0 ms, 0 ms, 251 ms (stage 4.0: task 22))'},\n",
       "     {'name': 'remote merged bytes read',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0.0 B (0.0 B, 0.0 B, 0.0 B (stage 11.0: task 2049))'},\n",
       "     {'name': 'local merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'corrupt merged block chunks', 'value': '0'},\n",
       "     {'name': 'remote merged reqs duration',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0 ms (0 ms, 0 ms, 0 ms (stage 11.0: task 2049))'},\n",
       "     {'name': 'remote merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'records read', 'value': '10,000,000'},\n",
       "     {'name': 'local bytes read',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n49.4 MiB (0.0 B, 0.0 B, 51.0 KiB (stage 8.0: task 985))'},\n",
       "     {'name': 'fetch wait time',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n34 ms (0 ms, 0 ms, 5 ms (stage 8.0: task 656))'},\n",
       "     {'name': 'remote bytes read',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0.0 B (0.0 B, 0.0 B, 0.0 B (stage 11.0: task 2049))'},\n",
       "     {'name': 'merged fetch fallback count', 'value': '0'},\n",
       "     {'name': 'local blocks read', 'value': '10,000'},\n",
       "     {'name': 'remote merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'remote blocks read', 'value': '0'},\n",
       "     {'name': 'data size',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n152.6 MiB (0.0 B, 0.0 B, 15.3 MiB (stage 4.0: task 22))'},\n",
       "     {'name': 'local merged bytes read',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0.0 B (0.0 B, 0.0 B, 0.0 B (stage 11.0: task 2049))'},\n",
       "     {'name': 'number of partitions', 'value': '1,000'},\n",
       "     {'name': 'remote reqs duration',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0 ms (0 ms, 0 ms, 0 ms (stage 11.0: task 2049))'},\n",
       "     {'name': 'remote bytes read to disk',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n0.0 B (0.0 B, 0.0 B, 0.0 B (stage 11.0: task 2049))'},\n",
       "     {'name': 'shuffle bytes written',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n49.4 MiB (0.0 B, 0.0 B, 5.1 MiB (stage 4.0: task 22))'}]},\n",
       "   {'nodeId': 8, 'nodeName': 'Union', 'metrics': []},\n",
       "   {'nodeId': 7, 'nodeName': 'AdaptiveSparkPlan', 'metrics': []},\n",
       "   {'nodeId': 6,\n",
       "    'nodeName': 'InMemoryTableScan',\n",
       "    'metrics': [{'name': 'number of output rows', 'value': '20,000,000'}]},\n",
       "   {'nodeId': 5,\n",
       "    'nodeName': 'HashAggregate',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': [{'name': 'time in aggregation build',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n502 ms (0 ms, 0 ms, 23 ms (stage 11.0: task 2041))'},\n",
       "     {'name': 'number of output rows', 'value': '2,000'}]},\n",
       "   {'nodeId': 4,\n",
       "    'nodeName': 'WholeStageCodegen (1)',\n",
       "    'metrics': [{'name': 'duration',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n1.7 s (0 ms, 0 ms, 29 ms (stage 11.0: task 2049))'}]},\n",
       "   {'nodeId': 3,\n",
       "    'nodeName': 'Exchange',\n",
       "    'metrics': [{'name': 'shuffle records written', 'value': '2,000'},\n",
       "     {'name': 'local merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'shuffle write time',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n1.9 s (0 ms, 0 ms, 16 ms (stage 11.0: task 2043))'},\n",
       "     {'name': 'remote merged bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'local merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'corrupt merged block chunks', 'value': '0'},\n",
       "     {'name': 'remote merged reqs duration', 'value': '0 ms'},\n",
       "     {'name': 'remote merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'records read', 'value': '2,000'},\n",
       "     {'name': 'local bytes read', 'value': '115.2 KiB'},\n",
       "     {'name': 'fetch wait time', 'value': '0 ms'},\n",
       "     {'name': 'remote bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'merged fetch fallback count', 'value': '0'},\n",
       "     {'name': 'local blocks read', 'value': '2,000'},\n",
       "     {'name': 'remote merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'remote blocks read', 'value': '0'},\n",
       "     {'name': 'data size',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n31.3 KiB (0.0 B, 16.0 B, 16.0 B (stage 11.0: task 2049))'},\n",
       "     {'name': 'local merged bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'number of partitions', 'value': '1'},\n",
       "     {'name': 'remote reqs duration', 'value': '0 ms'},\n",
       "     {'name': 'remote bytes read to disk', 'value': '0.0 B'},\n",
       "     {'name': 'shuffle bytes written',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n115.2 KiB (0.0 B, 59.0 B, 59.0 B (stage 11.0: task 2049))'}]},\n",
       "   {'nodeId': 2,\n",
       "    'nodeName': 'HashAggregate',\n",
       "    'wholeStageCodegenId': 2,\n",
       "    'metrics': [{'name': 'time in aggregation build', 'value': '46 ms'},\n",
       "     {'name': 'number of output rows', 'value': '1'}]},\n",
       "   {'nodeId': 1,\n",
       "    'nodeName': 'WholeStageCodegen (2)',\n",
       "    'metrics': [{'name': 'duration', 'value': '49 ms'}]},\n",
       "   {'nodeId': 0, 'nodeName': 'AdaptiveSparkPlan', 'metrics': []}],\n",
       "  'edges': [{'fromId': 2, 'toId': 0},\n",
       "   {'fromId': 3, 'toId': 2},\n",
       "   {'fromId': 5, 'toId': 3},\n",
       "   {'fromId': 6, 'toId': 5},\n",
       "   {'fromId': 7, 'toId': 6},\n",
       "   {'fromId': 8, 'toId': 7},\n",
       "   {'fromId': 9, 'toId': 8},\n",
       "   {'fromId': 10, 'toId': 9},\n",
       "   {'fromId': 12, 'toId': 10},\n",
       "   {'fromId': 13, 'toId': 12},\n",
       "   {'fromId': 14, 'toId': 8},\n",
       "   {'fromId': 15, 'toId': 14},\n",
       "   {'fromId': 17, 'toId': 15},\n",
       "   {'fromId': 18, 'toId': 17}]},\n",
       " {'id': 3,\n",
       "  'status': 'COMPLETED',\n",
       "  'description': 'scan tiny files',\n",
       "  'planDescription': '== Physical Plan ==\\nAdaptiveSparkPlan (12)\\n+- == Final Plan ==\\n   * HashAggregate (7)\\n   +- ShuffleQueryStage (6), Statistics(sizeInBytes=992.0 B, rowCount=62)\\n      +- Exchange (5)\\n         +- * HashAggregate (4)\\n            +- * Project (3)\\n               +- * ColumnarToRow (2)\\n                  +- Scan parquet  (1)\\n+- == Initial Plan ==\\n   HashAggregate (11)\\n   +- Exchange (10)\\n      +- HashAggregate (9)\\n         +- Project (8)\\n            +- Scan parquet  (1)\\n\\n\\n(1) Scan parquet \\nOutput [3]: [year#134, month#135, day#136]\\nBatched: true\\nLocation: InMemoryFileIndex [file:/Users/ugurkalkavan/Downloads/m06sparkbasics/weather]\\nPartitionFilters: [isnotnull(year#134), (year#134 = 2022)]\\nReadSchema: struct<>\\n\\n(2) ColumnarToRow [codegen id : 1]\\nInput [3]: [year#134, month#135, day#136]\\n\\n(3) Project [codegen id : 1]\\nOutput: []\\nInput [3]: [year#134, month#135, day#136]\\n\\n(4) HashAggregate [codegen id : 1]\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#157L]\\nResults [1]: [count#158L]\\n\\n(5) Exchange\\nInput [1]: [count#158L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=212]\\n\\n(6) ShuffleQueryStage\\nOutput [1]: [count#158L]\\nArguments: 0\\n\\n(7) HashAggregate [codegen id : 2]\\nInput [1]: [count#158L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#154L]\\nResults [1]: [count(1)#154L AS count#155L]\\n\\n(8) Project\\nOutput: []\\nInput [3]: [year#134, month#135, day#136]\\n\\n(9) HashAggregate\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#157L]\\nResults [1]: [count#158L]\\n\\n(10) Exchange\\nInput [1]: [count#158L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=196]\\n\\n(11) HashAggregate\\nInput [1]: [count#158L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#154L]\\nResults [1]: [count(1)#154L AS count#155L]\\n\\n(12) AdaptiveSparkPlan\\nOutput [1]: [count#155L]\\nArguments: isFinalPlan=true\\n\\n',\n",
       "  'submissionTime': '2024-06-06T21:52:44.377GMT',\n",
       "  'duration': 2894,\n",
       "  'runningJobIds': [],\n",
       "  'successJobIds': [10, 11],\n",
       "  'failedJobIds': [],\n",
       "  'nodes': [{'nodeId': 8,\n",
       "    'nodeName': 'Scan parquet',\n",
       "    'metrics': [{'name': 'number of files read', 'value': '1,101'},\n",
       "     {'name': 'scan time',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n15.4 s (67 ms, 202 ms, 690 ms (stage 19.0: task 4084))'},\n",
       "     {'name': 'dynamic partition pruning time', 'value': '0 ms'},\n",
       "     {'name': 'metadata time', 'value': '25 ms'},\n",
       "     {'name': 'size of files read', 'value': '3.5 GiB'},\n",
       "     {'name': 'number of output rows', 'value': '450,787,193'},\n",
       "     {'name': 'number of partitions read', 'value': '367'}]},\n",
       "   {'nodeId': 7,\n",
       "    'nodeName': 'ColumnarToRow',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': [{'name': 'number of output rows', 'value': '450,787,193'},\n",
       "     {'name': 'number of input batches', 'value': '110,398'}]},\n",
       "   {'nodeId': 6,\n",
       "    'nodeName': 'Project',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': []},\n",
       "   {'nodeId': 5,\n",
       "    'nodeName': 'HashAggregate',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': [{'name': 'time in aggregation build',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n18.0 s (81 ms, 215 ms, 869 ms (stage 19.0: task 4084))'},\n",
       "     {'name': 'number of output rows', 'value': '62'}]},\n",
       "   {'nodeId': 4,\n",
       "    'nodeName': 'WholeStageCodegen (1)',\n",
       "    'metrics': [{'name': 'duration',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n18.1 s (82 ms, 216 ms, 873 ms (stage 19.0: task 4084))'}]},\n",
       "   {'nodeId': 3,\n",
       "    'nodeName': 'Exchange',\n",
       "    'metrics': [{'name': 'shuffle records written', 'value': '62'},\n",
       "     {'name': 'local merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'shuffle write time',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n61 ms (0 ms, 0 ms, 13 ms (stage 19.0: task 4090))'},\n",
       "     {'name': 'remote merged bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'local merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'corrupt merged block chunks', 'value': '0'},\n",
       "     {'name': 'remote merged reqs duration', 'value': '0 ms'},\n",
       "     {'name': 'remote merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'records read', 'value': '62'},\n",
       "     {'name': 'local bytes read', 'value': '3.6 KiB'},\n",
       "     {'name': 'fetch wait time', 'value': '0 ms'},\n",
       "     {'name': 'remote bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'merged fetch fallback count', 'value': '0'},\n",
       "     {'name': 'local blocks read', 'value': '62'},\n",
       "     {'name': 'remote merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'remote blocks read', 'value': '0'},\n",
       "     {'name': 'data size',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n992.0 B (0.0 B, 16.0 B, 16.0 B (stage 19.0: task 4090))'},\n",
       "     {'name': 'local merged bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'number of partitions', 'value': '1'},\n",
       "     {'name': 'remote reqs duration', 'value': '0 ms'},\n",
       "     {'name': 'remote bytes read to disk', 'value': '0.0 B'},\n",
       "     {'name': 'shuffle bytes written',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n3.6 KiB (0.0 B, 59.0 B, 59.0 B (stage 19.0: task 4090))'}]},\n",
       "   {'nodeId': 2,\n",
       "    'nodeName': 'HashAggregate',\n",
       "    'wholeStageCodegenId': 2,\n",
       "    'metrics': [{'name': 'time in aggregation build', 'value': '6 ms'},\n",
       "     {'name': 'number of output rows', 'value': '1'}]},\n",
       "   {'nodeId': 1,\n",
       "    'nodeName': 'WholeStageCodegen (2)',\n",
       "    'metrics': [{'name': 'duration', 'value': '8 ms'}]},\n",
       "   {'nodeId': 0, 'nodeName': 'AdaptiveSparkPlan', 'metrics': []}],\n",
       "  'edges': [{'fromId': 2, 'toId': 0},\n",
       "   {'fromId': 3, 'toId': 2},\n",
       "   {'fromId': 5, 'toId': 3},\n",
       "   {'fromId': 6, 'toId': 5},\n",
       "   {'fromId': 7, 'toId': 6},\n",
       "   {'fromId': 8, 'toId': 7}]},\n",
       " {'id': 4,\n",
       "  'status': 'COMPLETED',\n",
       "  'description': 'scan tiny files 2',\n",
       "  'planDescription': '== Physical Plan ==\\nAdaptiveSparkPlan (10)\\n+- == Final Plan ==\\n   * HashAggregate (6)\\n   +- ShuffleQueryStage (5), Statistics(sizeInBytes=160.0 B, rowCount=10)\\n      +- Exchange (4)\\n         +- * HashAggregate (3)\\n            +- * ColumnarToRow (2)\\n               +- Scan parquet  (1)\\n+- == Initial Plan ==\\n   HashAggregate (9)\\n   +- Exchange (8)\\n      +- HashAggregate (7)\\n         +- Scan parquet  (1)\\n\\n\\n(1) Scan parquet \\nOutput: []\\nBatched: true\\nLocation: InMemoryFileIndex [file:/Users/ugurkalkavan/tmp/df_str]\\nReadSchema: struct<>\\n\\n(2) ColumnarToRow [codegen id : 1]\\nInput: []\\n\\n(3) HashAggregate [codegen id : 1]\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#165L]\\nResults [1]: [count#166L]\\n\\n(4) Exchange\\nInput [1]: [count#166L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=249]\\n\\n(5) ShuffleQueryStage\\nOutput [1]: [count#166L]\\nArguments: 0\\n\\n(6) HashAggregate [codegen id : 2]\\nInput [1]: [count#166L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#162L]\\nResults [1]: [count(1)#162L AS count#163L]\\n\\n(7) HashAggregate\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#165L]\\nResults [1]: [count#166L]\\n\\n(8) Exchange\\nInput [1]: [count#166L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=237]\\n\\n(9) HashAggregate\\nInput [1]: [count#166L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#162L]\\nResults [1]: [count(1)#162L AS count#163L]\\n\\n(10) AdaptiveSparkPlan\\nOutput [1]: [count#163L]\\nArguments: isFinalPlan=true\\n\\n',\n",
       "  'submissionTime': '2024-06-06T21:52:47.579GMT',\n",
       "  'duration': 462,\n",
       "  'runningJobIds': [],\n",
       "  'successJobIds': [13, 14],\n",
       "  'failedJobIds': [],\n",
       "  'nodes': [{'nodeId': 7,\n",
       "    'nodeName': 'Scan parquet',\n",
       "    'metrics': [{'name': 'number of files read', 'value': '1'},\n",
       "     {'name': 'scan time',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n902 ms (84 ms, 89 ms, 98 ms (stage 23.0: task 4152))'},\n",
       "     {'name': 'metadata time', 'value': '9 ms'},\n",
       "     {'name': 'size of files read', 'value': '290.5 MiB'},\n",
       "     {'name': 'number of output rows', 'value': '100,000,000'}]},\n",
       "   {'nodeId': 6,\n",
       "    'nodeName': 'ColumnarToRow',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': [{'name': 'number of output rows', 'value': '100,000,000'},\n",
       "     {'name': 'number of input batches', 'value': '24,416'}]},\n",
       "   {'nodeId': 5,\n",
       "    'nodeName': 'HashAggregate',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': [{'name': 'time in aggregation build',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n1.1 s (89 ms, 98 ms, 140 ms (stage 23.0: task 4153))'},\n",
       "     {'name': 'number of output rows', 'value': '10'}]},\n",
       "   {'nodeId': 4,\n",
       "    'nodeName': 'WholeStageCodegen (1)',\n",
       "    'metrics': [{'name': 'duration',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n1.1 s (98 ms, 99 ms, 142 ms (stage 23.0: task 4153))'}]},\n",
       "   {'nodeId': 3,\n",
       "    'nodeName': 'Exchange',\n",
       "    'metrics': [{'name': 'shuffle records written', 'value': '10'},\n",
       "     {'name': 'local merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'shuffle write time',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n41 ms (0 ms, 4 ms, 9 ms (stage 23.0: task 4147))'},\n",
       "     {'name': 'remote merged bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'local merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'corrupt merged block chunks', 'value': '0'},\n",
       "     {'name': 'remote merged reqs duration', 'value': '0 ms'},\n",
       "     {'name': 'remote merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'records read', 'value': '10'},\n",
       "     {'name': 'local bytes read', 'value': '569.0 B'},\n",
       "     {'name': 'fetch wait time', 'value': '0 ms'},\n",
       "     {'name': 'remote bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'merged fetch fallback count', 'value': '0'},\n",
       "     {'name': 'local blocks read', 'value': '10'},\n",
       "     {'name': 'remote merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'remote blocks read', 'value': '0'},\n",
       "     {'name': 'data size',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n160.0 B (0.0 B, 16.0 B, 16.0 B (stage 23.0: task 4148))'},\n",
       "     {'name': 'local merged bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'number of partitions', 'value': '1'},\n",
       "     {'name': 'remote reqs duration', 'value': '0 ms'},\n",
       "     {'name': 'remote bytes read to disk', 'value': '0.0 B'},\n",
       "     {'name': 'shuffle bytes written',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n569.0 B (0.0 B, 56.0 B, 59.0 B (stage 23.0: task 4156))'}]},\n",
       "   {'nodeId': 2,\n",
       "    'nodeName': 'HashAggregate',\n",
       "    'wholeStageCodegenId': 2,\n",
       "    'metrics': [{'name': 'time in aggregation build', 'value': '5 ms'},\n",
       "     {'name': 'number of output rows', 'value': '1'}]},\n",
       "   {'nodeId': 1,\n",
       "    'nodeName': 'WholeStageCodegen (2)',\n",
       "    'metrics': [{'name': 'duration', 'value': '6 ms'}]},\n",
       "   {'nodeId': 0, 'nodeName': 'AdaptiveSparkPlan', 'metrics': []}],\n",
       "  'edges': [{'fromId': 2, 'toId': 0},\n",
       "   {'fromId': 3, 'toId': 2},\n",
       "   {'fromId': 5, 'toId': 3},\n",
       "   {'fromId': 6, 'toId': 5},\n",
       "   {'fromId': 7, 'toId': 6}]},\n",
       " {'id': 5,\n",
       "  'status': 'COMPLETED',\n",
       "  'description': 'scan tiny files 3',\n",
       "  'planDescription': '== Physical Plan ==\\nAdaptiveSparkPlan (12)\\n+- == Final Plan ==\\n   * HashAggregate (7)\\n   +- ShuffleQueryStage (6), Statistics(sizeInBytes=7.7 KiB, rowCount=494)\\n      +- Exchange (5)\\n         +- * HashAggregate (4)\\n            +- * Project (3)\\n               +- * ColumnarToRow (2)\\n                  +- Scan parquet  (1)\\n+- == Initial Plan ==\\n   HashAggregate (11)\\n   +- Exchange (10)\\n      +- HashAggregate (9)\\n         +- Project (8)\\n            +- Scan parquet  (1)\\n\\n\\n(1) Scan parquet \\nOutput [3]: [year#172, month#173, day#174]\\nBatched: true\\nLocation: InMemoryFileIndex [file:/Users/ugurkalkavan/Downloads/m06sparkbasics/weather]\\nReadSchema: struct<>\\n\\n(2) ColumnarToRow [codegen id : 1]\\nInput [3]: [year#172, month#173, day#174]\\n\\n(3) Project [codegen id : 1]\\nOutput: []\\nInput [3]: [year#172, month#173, day#174]\\n\\n(4) HashAggregate [codegen id : 1]\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#194L]\\nResults [1]: [count#195L]\\n\\n(5) Exchange\\nInput [1]: [count#195L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=290]\\n\\n(6) ShuffleQueryStage\\nOutput [1]: [count#195L]\\nArguments: 0\\n\\n(7) HashAggregate [codegen id : 2]\\nInput [1]: [count#195L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#191L]\\nResults [1]: [count(1)#191L AS count#192L]\\n\\n(8) Project\\nOutput: []\\nInput [3]: [year#172, month#173, day#174]\\n\\n(9) HashAggregate\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#194L]\\nResults [1]: [count#195L]\\n\\n(10) Exchange\\nInput [1]: [count#195L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=274]\\n\\n(11) HashAggregate\\nInput [1]: [count#195L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#191L]\\nResults [1]: [count(1)#191L AS count#192L]\\n\\n(12) AdaptiveSparkPlan\\nOutput [1]: [count#192L]\\nArguments: isFinalPlan=true\\n\\n',\n",
       "  'submissionTime': '2024-06-06T21:52:59.767GMT',\n",
       "  'duration': 4153,\n",
       "  'runningJobIds': [],\n",
       "  'successJobIds': [16, 17],\n",
       "  'failedJobIds': [],\n",
       "  'nodes': [{'nodeId': 8,\n",
       "    'nodeName': 'Scan parquet',\n",
       "    'metrics': [{'name': 'number of files read', 'value': '8,796'},\n",
       "     {'name': 'scan time',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n25.8 s (0 ms, 43 ms, 203 ms (stage 27.0: task 4164))'},\n",
       "     {'name': 'dynamic partition pruning time', 'value': '0 ms'},\n",
       "     {'name': 'metadata time', 'value': '19 ms'},\n",
       "     {'name': 'size of files read', 'value': '27.7 GiB'},\n",
       "     {'name': 'number of output rows', 'value': '3,604,627,124'},\n",
       "     {'name': 'number of partitions read', 'value': '2,932'}]},\n",
       "   {'nodeId': 7,\n",
       "    'nodeName': 'ColumnarToRow',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': [{'name': 'number of output rows', 'value': '3,604,627,124'},\n",
       "     {'name': 'number of input batches', 'value': '882,616'}]},\n",
       "   {'nodeId': 6,\n",
       "    'nodeName': 'Project',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': []},\n",
       "   {'nodeId': 5,\n",
       "    'nodeName': 'HashAggregate',\n",
       "    'wholeStageCodegenId': 1,\n",
       "    'metrics': [{'name': 'time in aggregation build',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n31.7 s (9 ms, 56 ms, 212 ms (stage 27.0: task 4160))'},\n",
       "     {'name': 'number of output rows', 'value': '494'}]},\n",
       "   {'nodeId': 4,\n",
       "    'nodeName': 'WholeStageCodegen (1)',\n",
       "    'metrics': [{'name': 'duration',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n32.1 s (9 ms, 56 ms, 221 ms (stage 27.0: task 4163))'}]},\n",
       "   {'nodeId': 3,\n",
       "    'nodeName': 'Exchange',\n",
       "    'metrics': [{'name': 'shuffle records written', 'value': '494'},\n",
       "     {'name': 'local merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'shuffle write time',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n323 ms (0 ms, 0 ms, 14 ms (stage 27.0: task 4291))'},\n",
       "     {'name': 'remote merged bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'local merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'corrupt merged block chunks', 'value': '0'},\n",
       "     {'name': 'remote merged reqs duration', 'value': '0 ms'},\n",
       "     {'name': 'remote merged blocks fetched', 'value': '0'},\n",
       "     {'name': 'records read', 'value': '494'},\n",
       "     {'name': 'local bytes read', 'value': '28.5 KiB'},\n",
       "     {'name': 'fetch wait time', 'value': '0 ms'},\n",
       "     {'name': 'remote bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'merged fetch fallback count', 'value': '0'},\n",
       "     {'name': 'local blocks read', 'value': '494'},\n",
       "     {'name': 'remote merged chunks fetched', 'value': '0'},\n",
       "     {'name': 'remote blocks read', 'value': '0'},\n",
       "     {'name': 'data size',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n7.7 KiB (0.0 B, 16.0 B, 16.0 B (stage 27.0: task 4159))'},\n",
       "     {'name': 'local merged bytes read', 'value': '0.0 B'},\n",
       "     {'name': 'number of partitions', 'value': '1'},\n",
       "     {'name': 'remote reqs duration', 'value': '0 ms'},\n",
       "     {'name': 'remote bytes read to disk', 'value': '0.0 B'},\n",
       "     {'name': 'shuffle bytes written',\n",
       "      'value': 'total (min, med, max (stageId: taskId))\\n28.5 KiB (0.0 B, 59.0 B, 59.0 B (stage 27.0: task 4159))'}]},\n",
       "   {'nodeId': 2,\n",
       "    'nodeName': 'HashAggregate',\n",
       "    'wholeStageCodegenId': 2,\n",
       "    'metrics': [{'name': 'time in aggregation build', 'value': '23 ms'},\n",
       "     {'name': 'number of output rows', 'value': '1'}]},\n",
       "   {'nodeId': 1,\n",
       "    'nodeName': 'WholeStageCodegen (2)',\n",
       "    'metrics': [{'name': 'duration', 'value': '23 ms'}]},\n",
       "   {'nodeId': 0, 'nodeName': 'AdaptiveSparkPlan', 'metrics': []}],\n",
       "  'edges': [{'fromId': 2, 'toId': 0},\n",
       "   {'fromId': 3, 'toId': 2},\n",
       "   {'fromId': 5, 'toId': 3},\n",
       "   {'fromId': 6, 'toId': 5},\n",
       "   {'fromId': 7, 'toId': 6},\n",
       "   {'fromId': 8, 'toId': 7}]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ba93d-430f-46c7-b9d0-05c4104d0d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
