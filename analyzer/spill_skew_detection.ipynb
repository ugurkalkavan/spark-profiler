{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0d7aa-8dfa-4c49-80ae-717e6a15b8e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:05:45.883070Z",
     "iopub.status.busy": "2024-06-06T10:05:45.882913Z",
     "iopub.status.idle": "2024-06-06T10:05:49.893961Z",
     "shell.execute_reply": "2024-06-06T10:05:49.892971Z",
     "shell.execute_reply.started": "2024-06-06T10:05:45.883059Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spillExample').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c0fc8f6-177a-418b-8d7a-a630a8bc18d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:39:07.115258Z",
     "iopub.status.busy": "2024-06-05T14:39:07.113903Z",
     "iopub.status.idle": "2024-06-05T14:39:40.881154Z",
     "shell.execute_reply": "2024-06-05T14:39:40.880573Z",
     "shell.execute_reply.started": "2024-06-05T14:39:07.115198Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:39:15 WARN TaskSetManager: Stage 0 contains a task of very large size (6737 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/05 17:39:20 WARN TaskSetManager: Stage 1 contains a task of very large size (6737 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/05 17:39:22 WARN TaskSetManager: Stage 4 contains a task of very large size (6737 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/05 17:39:27 WARN TaskSetManager: Stage 5 contains a task of very large size (6737 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark.sparkContext.setJobDescription('spill')  # Setting Job description\n",
    "data = range(0, 10000000)\n",
    "# Create DataFrame with a lot of records\n",
    "df = spark.createDataFrame(data, \"int\").toDF(\"id\")\n",
    "df.cache().count()\n",
    "\n",
    "# Create a temporary view so we can run SQL queries\n",
    "df.createOrReplaceTempView(\"records\")\n",
    "\n",
    "# Force a spill. The high number of partitions and usage of `UNION ALL`\n",
    "# will generate a big number of records in memory\n",
    "df2 = spark.sql(\"\"\"\n",
    "    SELECT /*+ REPARTITION(1000) */ id \n",
    "    FROM records \n",
    "    UNION ALL \n",
    "    SELECT /*+ REPARTITION(1000) */ id \n",
    "    FROM records\n",
    "\"\"\")\n",
    "df2.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73706b7f-74f5-46db-9024-b5f220106746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:40:27.737886Z",
     "iopub.status.busy": "2024-06-06T10:40:27.737065Z",
     "iopub.status.idle": "2024-06-06T10:40:27.793801Z",
     "shell.execute_reply": "2024-06-06T10:40:27.793203Z",
     "shell.execute_reply.started": "2024-06-06T10:40:27.737843Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.12:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spillExample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1414635d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e6608e-3c99-4504-86d8-71b7c32b7832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col\n",
    "import random\n",
    "\n",
    "# Spark session oluşturuluyor\n",
    "spark.sparkContext.setJobDescription('skew')  # Setting Job description\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Büyük bir RDD yaratılıyor\n",
    "rdd = sc.parallelize([(i, random.randint(0,1000000)) for i in range(10000000)])\n",
    "\n",
    "# Key'ler bazen aynı olacak şekilde RDD'ye bozuk veri ekleniyor\n",
    "rdd = rdd.union(sc.parallelize([(0, i) for i in range(1000000)]))\n",
    "\n",
    "# Reduce operasyonunda shuffle ve skewed data oluştuğundan emin olunur\n",
    "result_rdd = rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "result_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "2c1d33de-513f-4973-ab0b-8ef4ac3d405b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T12:37:11.105188Z",
     "iopub.status.busy": "2024-06-06T12:37:11.104631Z",
     "iopub.status.idle": "2024-06-06T12:37:13.845774Z",
     "shell.execute_reply": "2024-06-06T12:37:13.845244Z",
     "shell.execute_reply.started": "2024-06-06T12:37:11.105147Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450787193"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.setJobDescription('scan tiny files')  # Setting Job description\n",
    "\n",
    "weather = spark.read.parquet(\"/Users/ugurkalkavan/Downloads/m06sparkbasics/weather\")\n",
    "weather.filter(weather[\"year\"]==2022).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "b2cd212d-fb5e-474e-a239-e8913fc8f95e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T13:27:26.728534Z",
     "iopub.status.busy": "2024-06-06T13:27:26.726731Z",
     "iopub.status.idle": "2024-06-06T13:27:27.965940Z",
     "shell.execute_reply": "2024-06-06T13:27:27.965569Z",
     "shell.execute_reply.started": "2024-06-06T13:27:26.728450Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.setJobDescription('scan tiny files 2')  # Setting Job description\n",
    "\n",
    "df_str = spark.read.parquet(\"/Users/ugurkalkavan/tmp/df_str\")\n",
    "df_str.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73bc6d85-b88d-4779-a876-984eef74d97f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T21:57:54.820602Z",
     "iopub.status.busy": "2024-06-06T21:57:54.818739Z",
     "iopub.status.idle": "2024-06-06T21:57:54.829127Z",
     "shell.execute_reply": "2024-06-06T21:57:54.828491Z",
     "shell.execute_reply.started": "2024-06-06T21:57:54.820475Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "faf0a241-1248-40f0-85da-cc6f60414c71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:47:20.917224Z",
     "iopub.status.busy": "2024-06-06T10:47:20.915727Z",
     "iopub.status.idle": "2024-06-06T10:47:20.923659Z",
     "shell.execute_reply": "2024-06-06T10:47:20.922930Z",
     "shell.execute_reply.started": "2024-06-06T10:47:20.917147Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Spark UIs API endpoint, 'local' is Spark's default master URL, replace 'local' with your Spark master\n",
    "SPARK_API_ENDPOINT = \"http://localhost:4041/api/v1/applications\"\n",
    "\n",
    "def fetch_spark_apps(spark):\n",
    "    try:\n",
    "        \n",
    "        app_id = spark.sparkContext.applicationId\n",
    "        return app_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fetching Spark Apps: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690cd6c8-fd4a-4bd9-894e-9e95e9f33154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:47:21.305511Z",
     "iopub.status.busy": "2024-06-06T10:47:21.304788Z",
     "iopub.status.idle": "2024-06-06T10:47:21.313232Z",
     "shell.execute_reply": "2024-06-06T10:47:21.312382Z",
     "shell.execute_reply.started": "2024-06-06T10:47:21.305458Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sizeof_fmt(num, suffix=\"B\"):\n",
    "    for unit in (\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"):\n",
    "        if abs(num) < 1024.0:\n",
    "            return f\"{num:3.1f} {unit}{suffix}\"\n",
    "        num /= 1024.0\n",
    "    return f\"{num:.1f}Yi{suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "56d1035c-278a-4cf0-bc17-79c490d3c97d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T12:13:29.225799Z",
     "iopub.status.busy": "2024-06-06T12:13:29.224753Z",
     "iopub.status.idle": "2024-06-06T12:13:29.235452Z",
     "shell.execute_reply": "2024-06-06T12:13:29.234748Z",
     "shell.execute_reply.started": "2024-06-06T12:13:29.225754Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_size(size_string, suffix=\"B\"):\n",
    "    units = {\"\": 0, \"Ki\": 1, \"Mi\": 2, \"Gi\": 3, \"Ti\": 4, \"Pi\": 5, \"Ei\": 6, \"Zi\": 7, \"Yi\": 8}\n",
    "    \n",
    "    # Boşlukla ayrılmış son iki değeri al\n",
    "    size, unit = size_string.split()[-2:]\n",
    "    size = float(size)\n",
    "\n",
    "    # İlgili çarpanı bulmak için sözlükten ilgili uniti al\n",
    "    power = units[unit.replace(suffix, '')]\n",
    "\n",
    "    # İlgili çarpan kadar 1024 ile çarp\n",
    "    return size * (1024 ** power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "537093cf-259a-46fc-a20b-e35b651d1163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:47:21.619830Z",
     "iopub.status.busy": "2024-06-06T10:47:21.619118Z",
     "iopub.status.idle": "2024-06-06T10:47:21.624393Z",
     "shell.execute_reply": "2024-06-06T10:47:21.623899Z",
     "shell.execute_reply.started": "2024-06-06T10:47:21.619783Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_scaler(data):\n",
    "    data_min = min(data)\n",
    "    data_range = max(data) - data_min\n",
    "    scaled_data = [(item-data_min)/data_range for item in data]\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2850bd05-07e6-44bc-afbb-aa136d45ba17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:47:21.821797Z",
     "iopub.status.busy": "2024-06-06T10:47:21.821361Z",
     "iopub.status.idle": "2024-06-06T10:47:21.829302Z",
     "shell.execute_reply": "2024-06-06T10:47:21.828664Z",
     "shell.execute_reply.started": "2024-06-06T10:47:21.821739Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_anomalies(data, m=1.4):\n",
    "    data_min = min(data)\n",
    "    data_range = max(data) - data_min\n",
    "    if data_range == 0:  # if all data points are the same\n",
    "        return False\n",
    "    else:\n",
    "        scaled_data = [(item - data_min) / data_range for item in data]\n",
    "\n",
    "        # Detect outliers\n",
    "        mean = np.mean(scaled_data)\n",
    "        std_dev = np.std(scaled_data)\n",
    "\n",
    "        return not any((abs(mean - value) > m * std_dev) for value in scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "2a91f566-ab7a-4815-ba81-8c9086fbea34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T14:09:21.529672Z",
     "iopub.status.busy": "2024-06-06T14:09:21.528258Z",
     "iopub.status.idle": "2024-06-06T14:09:21.540819Z",
     "shell.execute_reply": "2024-06-06T14:09:21.540113Z",
     "shell.execute_reply.started": "2024-06-06T14:09:21.529610Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scan_parquet_metrics(app_id):\n",
    "    metrics_dict = {}\n",
    "    try:\n",
    "        response = requests.get(f\"{SPARK_API_ENDPOINT}/{app_id}/sql\")\n",
    "        sql = json.loads(response.text)\n",
    "        print(response.text)\n",
    "        for item in sql:\n",
    "            sql_id = item['id']\n",
    "            metrics_dict[sql_id] = {}\n",
    "            for node in item['nodes']:\n",
    "                if node['nodeName'] == 'Scan parquet':\n",
    "                    node_metrics = {}\n",
    "                    for metric in node['metrics']:\n",
    "                        node_metrics[metric['name']] = metric['value']\n",
    "                    metrics_dict[sql_id] = node_metrics    \n",
    "        return metrics_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fetching metrics: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "8a802534-24d5-4a1d-8e2f-add9686011c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T14:09:21.948326Z",
     "iopub.status.busy": "2024-06-06T14:09:21.947628Z",
     "iopub.status.idle": "2024-06-06T14:09:21.959009Z",
     "shell.execute_reply": "2024-06-06T14:09:21.958419Z",
     "shell.execute_reply.started": "2024-06-06T14:09:21.948274Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_tiny_files(metrics_dict: dict):\n",
    "    for sql_id, metrics in metrics_dict.items():\n",
    "        print(metrics)\n",
    "        \n",
    "        print(f\"Processing SQL_ID: {sql_id}\")\n",
    "        \n",
    "        files_read = int(metrics['number of files read'].replace(',', '')) if isinstance(metrics['number of files read'], str) else metrics['number of files read']\n",
    "       \n",
    "        # 'number of partitions read' metriği yoksa, değeri 1 olarak ayarla\n",
    "        partitions_read = 1\n",
    "        if 'number of partitions read' in metrics:\n",
    "            partitions_read = int(metrics['number of partitions read'].replace(',', '')) if isinstance(metrics['number of partitions read'], str) else metrics['number of partitions read']\n",
    "\n",
    "\n",
    "        size_read = convert_size(metrics['size of files read'])\n",
    "        \n",
    "        average_number_of_file_by_partition = files_read / partitions_read\n",
    "        average_file_size = size_read / files_read\n",
    "        \n",
    "        # Metriklerin değerlerini kontrol et\n",
    "        print(\"Number of files read:            \", metrics['number of files read'])\n",
    "        print(\"Scan time:                       \", metrics['scan time'])\n",
    "        if 'Dynamic partition pruning time' in metrics:\n",
    "            print(\"Dynamic partition pruning time:  \", metrics['dynamic partition pruning time'])\n",
    "        print(\"Metadata time:                   \", metrics['metadata time'])\n",
    "        print(\"Size of files read:              \", metrics['size of files read'])\n",
    "        print(\"Number of output rows:           \", metrics['number of output rows'])\n",
    "        if 'Number of partitions read' in metrics:\n",
    "            print(\"Dynamic partition pruning time:  \", metrics['number of partitions read'])\n",
    "        \n",
    "        print(f\"Average number of files by partition: {average_number_of_file_by_partition}\")\n",
    "        print(f\"Average file size: {sizeof_fmt(average_file_size)}\")\n",
    "        \n",
    "        if average_number_of_file_by_partition > 1 and size_read/partitions_read < 134217728 :\n",
    "            print(\"Tiny file PROBLEM!!!\")\n",
    "\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "4cabf288-8299-46c7-a25f-2d5a2db533e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T14:09:22.606437Z",
     "iopub.status.busy": "2024-06-06T14:09:22.606165Z",
     "iopub.status.idle": "2024-06-06T14:09:22.672566Z",
     "shell.execute_reply": "2024-06-06T14:09:22.672110Z",
     "shell.execute_reply.started": "2024-06-06T14:09:22.606419Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ {\n",
      "  \"id\" : 0,\n",
      "  \"status\" : \"COMPLETED\",\n",
      "  \"description\" : \"scan tiny files\",\n",
      "  \"planDescription\" : \"== Physical Plan ==\\nAdaptiveSparkPlan (12)\\n+- == Final Plan ==\\n   * HashAggregate (7)\\n   +- ShuffleQueryStage (6), Statistics(sizeInBytes=7.7 KiB, rowCount=494)\\n      +- Exchange (5)\\n         +- * HashAggregate (4)\\n            +- * Project (3)\\n               +- * ColumnarToRow (2)\\n                  +- Scan parquet  (1)\\n+- == Initial Plan ==\\n   HashAggregate (11)\\n   +- Exchange (10)\\n      +- HashAggregate (9)\\n         +- Project (8)\\n            +- Scan parquet  (1)\\n\\n\\n(1) Scan parquet \\nOutput [3]: [year#5, month#6, day#7]\\nBatched: true\\nLocation: InMemoryFileIndex [file:/Users/ugurkalkavan/Downloads/m06sparkbasics/weather]\\nReadSchema: struct<>\\n\\n(2) ColumnarToRow [codegen id : 1]\\nInput [3]: [year#5, month#6, day#7]\\n\\n(3) Project [codegen id : 1]\\nOutput: []\\nInput [3]: [year#5, month#6, day#7]\\n\\n(4) HashAggregate [codegen id : 1]\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#27L]\\nResults [1]: [count#28L]\\n\\n(5) Exchange\\nInput [1]: [count#28L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=29]\\n\\n(6) ShuffleQueryStage\\nOutput [1]: [count#28L]\\nArguments: 0\\n\\n(7) HashAggregate [codegen id : 2]\\nInput [1]: [count#28L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#24L]\\nResults [1]: [count(1)#24L AS count#25L]\\n\\n(8) Project\\nOutput: []\\nInput [3]: [year#5, month#6, day#7]\\n\\n(9) HashAggregate\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#27L]\\nResults [1]: [count#28L]\\n\\n(10) Exchange\\nInput [1]: [count#28L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=13]\\n\\n(11) HashAggregate\\nInput [1]: [count#28L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#24L]\\nResults [1]: [count(1)#24L AS count#25L]\\n\\n(12) AdaptiveSparkPlan\\nOutput [1]: [count#25L]\\nArguments: isFinalPlan=true\\n\\n\",\n",
      "  \"submissionTime\" : \"2024-06-06T10:07:44.423GMT\",\n",
      "  \"duration\" : 8446,\n",
      "  \"runningJobIds\" : [ ],\n",
      "  \"successJobIds\" : [ 1, 2 ],\n",
      "  \"failedJobIds\" : [ ],\n",
      "  \"nodes\" : [ {\n",
      "    \"nodeId\" : 8,\n",
      "    \"nodeName\" : \"Scan parquet\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"number of files read\",\n",
      "      \"value\" : \"8,796\"\n",
      "    }, {\n",
      "      \"name\" : \"scan time\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n39.9 s (6 ms, 51 ms, 655 ms (stage 1.0: task 3))\"\n",
      "    }, {\n",
      "      \"name\" : \"dynamic partition pruning time\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"metadata time\",\n",
      "      \"value\" : \"38 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"size of files read\",\n",
      "      \"value\" : \"27.7 GiB\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"3,604,627,124\"\n",
      "    }, {\n",
      "      \"name\" : \"number of partitions read\",\n",
      "      \"value\" : \"2,932\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 7,\n",
      "    \"nodeName\" : \"ColumnarToRow\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"3,604,627,124\"\n",
      "    }, {\n",
      "      \"name\" : \"number of input batches\",\n",
      "      \"value\" : \"882,616\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 6,\n",
      "    \"nodeName\" : \"Project\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ ]\n",
      "  }, {\n",
      "    \"nodeId\" : 5,\n",
      "    \"nodeName\" : \"HashAggregate\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"time in aggregation build\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n46.8 s (14 ms, 61 ms, 812 ms (stage 1.0: task 1))\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"494\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 4,\n",
      "    \"nodeName\" : \"WholeStageCodegen (1)\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"duration\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n48.0 s (16 ms, 62 ms, 860 ms (stage 1.0: task 1))\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 3,\n",
      "    \"nodeName\" : \"Exchange\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"shuffle records written\",\n",
      "      \"value\" : \"494\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged chunks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"shuffle write time\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n1.1 s (0 ms, 0 ms, 94 ms (stage 1.0: task 5))\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged blocks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"corrupt merged block chunks\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged reqs duration\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged blocks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"records read\",\n",
      "      \"value\" : \"494\"\n",
      "    }, {\n",
      "      \"name\" : \"local bytes read\",\n",
      "      \"value\" : \"28.5 KiB\"\n",
      "    }, {\n",
      "      \"name\" : \"fetch wait time\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"merged fetch fallback count\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"local blocks read\",\n",
      "      \"value\" : \"494\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged chunks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"remote blocks read\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"data size\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n7.7 KiB (0.0 B, 16.0 B, 16.0 B (stage 1.0: task 1))\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"number of partitions\",\n",
      "      \"value\" : \"1\"\n",
      "    }, {\n",
      "      \"name\" : \"remote reqs duration\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote bytes read to disk\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"shuffle bytes written\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n28.5 KiB (0.0 B, 59.0 B, 59.0 B (stage 1.0: task 1))\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 2,\n",
      "    \"nodeName\" : \"HashAggregate\",\n",
      "    \"wholeStageCodegenId\" : 2,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"time in aggregation build\",\n",
      "      \"value\" : \"40 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"1\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 1,\n",
      "    \"nodeName\" : \"WholeStageCodegen (2)\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"duration\",\n",
      "      \"value\" : \"41 ms\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 0,\n",
      "    \"nodeName\" : \"AdaptiveSparkPlan\",\n",
      "    \"metrics\" : [ ]\n",
      "  } ],\n",
      "  \"edges\" : [ {\n",
      "    \"fromId\" : 2,\n",
      "    \"toId\" : 0\n",
      "  }, {\n",
      "    \"fromId\" : 3,\n",
      "    \"toId\" : 2\n",
      "  }, {\n",
      "    \"fromId\" : 5,\n",
      "    \"toId\" : 3\n",
      "  }, {\n",
      "    \"fromId\" : 6,\n",
      "    \"toId\" : 5\n",
      "  }, {\n",
      "    \"fromId\" : 7,\n",
      "    \"toId\" : 6\n",
      "  }, {\n",
      "    \"fromId\" : 8,\n",
      "    \"toId\" : 7\n",
      "  } ]\n",
      "}, {\n",
      "  \"id\" : 1,\n",
      "  \"status\" : \"COMPLETED\",\n",
      "  \"description\" : \"scan tiny files\",\n",
      "  \"planDescription\" : \"== Physical Plan ==\\nAdaptiveSparkPlan (12)\\n+- == Final Plan ==\\n   * HashAggregate (7)\\n   +- ShuffleQueryStage (6), Statistics(sizeInBytes=7.7 KiB, rowCount=494)\\n      +- Exchange (5)\\n         +- * HashAggregate (4)\\n            +- * Project (3)\\n               +- * ColumnarToRow (2)\\n                  +- Scan parquet  (1)\\n+- == Initial Plan ==\\n   HashAggregate (11)\\n   +- Exchange (10)\\n      +- HashAggregate (9)\\n         +- Project (8)\\n            +- Scan parquet  (1)\\n\\n\\n(1) Scan parquet \\nOutput [3]: [year#34, month#35, day#36]\\nBatched: true\\nLocation: InMemoryFileIndex [file:/Users/ugurkalkavan/Downloads/m06sparkbasics/weather]\\nReadSchema: struct<>\\n\\n(2) ColumnarToRow [codegen id : 1]\\nInput [3]: [year#34, month#35, day#36]\\n\\n(3) Project [codegen id : 1]\\nOutput: []\\nInput [3]: [year#34, month#35, day#36]\\n\\n(4) HashAggregate [codegen id : 1]\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#56L]\\nResults [1]: [count#57L]\\n\\n(5) Exchange\\nInput [1]: [count#57L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=72]\\n\\n(6) ShuffleQueryStage\\nOutput [1]: [count#57L]\\nArguments: 0\\n\\n(7) HashAggregate [codegen id : 2]\\nInput [1]: [count#57L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#53L]\\nResults [1]: [count(1)#53L AS count#54L]\\n\\n(8) Project\\nOutput: []\\nInput [3]: [year#34, month#35, day#36]\\n\\n(9) HashAggregate\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#56L]\\nResults [1]: [count#57L]\\n\\n(10) Exchange\\nInput [1]: [count#57L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=56]\\n\\n(11) HashAggregate\\nInput [1]: [count#57L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#53L]\\nResults [1]: [count(1)#53L AS count#54L]\\n\\n(12) AdaptiveSparkPlan\\nOutput [1]: [count#54L]\\nArguments: isFinalPlan=true\\n\\n\",\n",
      "  \"submissionTime\" : \"2024-06-06T12:35:04.045GMT\",\n",
      "  \"duration\" : 4044,\n",
      "  \"runningJobIds\" : [ ],\n",
      "  \"successJobIds\" : [ 4, 5 ],\n",
      "  \"failedJobIds\" : [ ],\n",
      "  \"nodes\" : [ {\n",
      "    \"nodeId\" : 8,\n",
      "    \"nodeName\" : \"Scan parquet\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"number of files read\",\n",
      "      \"value\" : \"8,796\"\n",
      "    }, {\n",
      "      \"name\" : \"scan time\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n17.3 s (1 ms, 25 ms, 387 ms (stage 5.0: task 501))\"\n",
      "    }, {\n",
      "      \"name\" : \"dynamic partition pruning time\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"metadata time\",\n",
      "      \"value\" : \"11 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"size of files read\",\n",
      "      \"value\" : \"27.7 GiB\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"3,604,627,124\"\n",
      "    }, {\n",
      "      \"name\" : \"number of partitions read\",\n",
      "      \"value\" : \"2,932\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 7,\n",
      "    \"nodeName\" : \"ColumnarToRow\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"3,604,627,124\"\n",
      "    }, {\n",
      "      \"name\" : \"number of input batches\",\n",
      "      \"value\" : \"882,616\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 6,\n",
      "    \"nodeName\" : \"Project\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ ]\n",
      "  }, {\n",
      "    \"nodeId\" : 5,\n",
      "    \"nodeName\" : \"HashAggregate\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"time in aggregation build\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n22.6 s (10 ms, 37 ms, 398 ms (stage 5.0: task 501))\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"494\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 4,\n",
      "    \"nodeName\" : \"WholeStageCodegen (1)\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"duration\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n23.4 s (10 ms, 38 ms, 402 ms (stage 5.0: task 501))\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 3,\n",
      "    \"nodeName\" : \"Exchange\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"shuffle records written\",\n",
      "      \"value\" : \"494\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged chunks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"shuffle write time\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n611 ms (0 ms, 0 ms, 19 ms (stage 5.0: task 512))\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged blocks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"corrupt merged block chunks\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged reqs duration\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged blocks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"records read\",\n",
      "      \"value\" : \"494\"\n",
      "    }, {\n",
      "      \"name\" : \"local bytes read\",\n",
      "      \"value\" : \"28.5 KiB\"\n",
      "    }, {\n",
      "      \"name\" : \"fetch wait time\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"merged fetch fallback count\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"local blocks read\",\n",
      "      \"value\" : \"494\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged chunks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"remote blocks read\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"data size\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n7.7 KiB (0.0 B, 16.0 B, 16.0 B (stage 5.0: task 503))\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"number of partitions\",\n",
      "      \"value\" : \"1\"\n",
      "    }, {\n",
      "      \"name\" : \"remote reqs duration\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote bytes read to disk\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"shuffle bytes written\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n28.5 KiB (0.0 B, 59.0 B, 59.0 B (stage 5.0: task 503))\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 2,\n",
      "    \"nodeName\" : \"HashAggregate\",\n",
      "    \"wholeStageCodegenId\" : 2,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"time in aggregation build\",\n",
      "      \"value\" : \"32 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"1\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 1,\n",
      "    \"nodeName\" : \"WholeStageCodegen (2)\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"duration\",\n",
      "      \"value\" : \"33 ms\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 0,\n",
      "    \"nodeName\" : \"AdaptiveSparkPlan\",\n",
      "    \"metrics\" : [ ]\n",
      "  } ],\n",
      "  \"edges\" : [ {\n",
      "    \"fromId\" : 2,\n",
      "    \"toId\" : 0\n",
      "  }, {\n",
      "    \"fromId\" : 3,\n",
      "    \"toId\" : 2\n",
      "  }, {\n",
      "    \"fromId\" : 5,\n",
      "    \"toId\" : 3\n",
      "  }, {\n",
      "    \"fromId\" : 6,\n",
      "    \"toId\" : 5\n",
      "  }, {\n",
      "    \"fromId\" : 7,\n",
      "    \"toId\" : 6\n",
      "  }, {\n",
      "    \"fromId\" : 8,\n",
      "    \"toId\" : 7\n",
      "  } ]\n",
      "}, {\n",
      "  \"id\" : 2,\n",
      "  \"status\" : \"COMPLETED\",\n",
      "  \"description\" : \"scan tiny files\",\n",
      "  \"planDescription\" : \"== Physical Plan ==\\nAdaptiveSparkPlan (12)\\n+- == Final Plan ==\\n   * HashAggregate (7)\\n   +- ShuffleQueryStage (6), Statistics(sizeInBytes=992.0 B, rowCount=62)\\n      +- Exchange (5)\\n         +- * HashAggregate (4)\\n            +- * Project (3)\\n               +- * ColumnarToRow (2)\\n                  +- Scan parquet  (1)\\n+- == Initial Plan ==\\n   HashAggregate (11)\\n   +- Exchange (10)\\n      +- HashAggregate (9)\\n         +- Project (8)\\n            +- Scan parquet  (1)\\n\\n\\n(1) Scan parquet \\nOutput [3]: [year#63, month#64, day#65]\\nBatched: true\\nLocation: InMemoryFileIndex [file:/Users/ugurkalkavan/Downloads/m06sparkbasics/weather]\\nPartitionFilters: [isnotnull(year#63), (year#63 = 2022)]\\nReadSchema: struct<>\\n\\n(2) ColumnarToRow [codegen id : 1]\\nInput [3]: [year#63, month#64, day#65]\\n\\n(3) Project [codegen id : 1]\\nOutput: []\\nInput [3]: [year#63, month#64, day#65]\\n\\n(4) HashAggregate [codegen id : 1]\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#86L]\\nResults [1]: [count#87L]\\n\\n(5) Exchange\\nInput [1]: [count#87L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=115]\\n\\n(6) ShuffleQueryStage\\nOutput [1]: [count#87L]\\nArguments: 0\\n\\n(7) HashAggregate [codegen id : 2]\\nInput [1]: [count#87L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#83L]\\nResults [1]: [count(1)#83L AS count#84L]\\n\\n(8) Project\\nOutput: []\\nInput [3]: [year#63, month#64, day#65]\\n\\n(9) HashAggregate\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#86L]\\nResults [1]: [count#87L]\\n\\n(10) Exchange\\nInput [1]: [count#87L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=99]\\n\\n(11) HashAggregate\\nInput [1]: [count#87L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#83L]\\nResults [1]: [count(1)#83L AS count#84L]\\n\\n(12) AdaptiveSparkPlan\\nOutput [1]: [count#84L]\\nArguments: isFinalPlan=true\\n\\n\",\n",
      "  \"submissionTime\" : \"2024-06-06T12:37:13.066GMT\",\n",
      "  \"duration\" : 775,\n",
      "  \"runningJobIds\" : [ ],\n",
      "  \"successJobIds\" : [ 7, 8 ],\n",
      "  \"failedJobIds\" : [ ],\n",
      "  \"nodes\" : [ {\n",
      "    \"nodeId\" : 8,\n",
      "    \"nodeName\" : \"Scan parquet\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"number of files read\",\n",
      "      \"value\" : \"1,101\"\n",
      "    }, {\n",
      "      \"name\" : \"scan time\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n1.8 s (8 ms, 28 ms, 58 ms (stage 9.0: task 999))\"\n",
      "    }, {\n",
      "      \"name\" : \"dynamic partition pruning time\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"metadata time\",\n",
      "      \"value\" : \"24 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"size of files read\",\n",
      "      \"value\" : \"3.5 GiB\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"450,787,193\"\n",
      "    }, {\n",
      "      \"name\" : \"number of partitions read\",\n",
      "      \"value\" : \"367\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 7,\n",
      "    \"nodeName\" : \"ColumnarToRow\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"450,787,193\"\n",
      "    }, {\n",
      "      \"name\" : \"number of input batches\",\n",
      "      \"value\" : \"110,398\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 6,\n",
      "    \"nodeName\" : \"Project\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ ]\n",
      "  }, {\n",
      "    \"nodeId\" : 5,\n",
      "    \"nodeName\" : \"HashAggregate\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"time in aggregation build\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n2.5 s (16 ms, 40 ms, 68 ms (stage 9.0: task 1001))\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"62\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 4,\n",
      "    \"nodeName\" : \"WholeStageCodegen (1)\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"duration\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n2.7 s (16 ms, 40 ms, 76 ms (stage 9.0: task 1002))\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 3,\n",
      "    \"nodeName\" : \"Exchange\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"shuffle records written\",\n",
      "      \"value\" : \"62\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged chunks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"shuffle write time\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n96 ms (0 ms, 0 ms, 9 ms (stage 9.0: task 996))\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged blocks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"corrupt merged block chunks\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged reqs duration\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged blocks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"records read\",\n",
      "      \"value\" : \"62\"\n",
      "    }, {\n",
      "      \"name\" : \"local bytes read\",\n",
      "      \"value\" : \"3.6 KiB\"\n",
      "    }, {\n",
      "      \"name\" : \"fetch wait time\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"merged fetch fallback count\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"local blocks read\",\n",
      "      \"value\" : \"62\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged chunks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"remote blocks read\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"data size\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n992.0 B (0.0 B, 16.0 B, 16.0 B (stage 9.0: task 1002))\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"number of partitions\",\n",
      "      \"value\" : \"1\"\n",
      "    }, {\n",
      "      \"name\" : \"remote reqs duration\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote bytes read to disk\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"shuffle bytes written\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n3.6 KiB (0.0 B, 59.0 B, 59.0 B (stage 9.0: task 1002))\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 2,\n",
      "    \"nodeName\" : \"HashAggregate\",\n",
      "    \"wholeStageCodegenId\" : 2,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"time in aggregation build\",\n",
      "      \"value\" : \"7 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"1\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 1,\n",
      "    \"nodeName\" : \"WholeStageCodegen (2)\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"duration\",\n",
      "      \"value\" : \"9 ms\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 0,\n",
      "    \"nodeName\" : \"AdaptiveSparkPlan\",\n",
      "    \"metrics\" : [ ]\n",
      "  } ],\n",
      "  \"edges\" : [ {\n",
      "    \"fromId\" : 2,\n",
      "    \"toId\" : 0\n",
      "  }, {\n",
      "    \"fromId\" : 3,\n",
      "    \"toId\" : 2\n",
      "  }, {\n",
      "    \"fromId\" : 5,\n",
      "    \"toId\" : 3\n",
      "  }, {\n",
      "    \"fromId\" : 6,\n",
      "    \"toId\" : 5\n",
      "  }, {\n",
      "    \"fromId\" : 7,\n",
      "    \"toId\" : 6\n",
      "  }, {\n",
      "    \"fromId\" : 8,\n",
      "    \"toId\" : 7\n",
      "  } ]\n",
      "}, {\n",
      "  \"id\" : 3,\n",
      "  \"status\" : \"COMPLETED\",\n",
      "  \"description\" : \"scan tiny files 2\",\n",
      "  \"planDescription\" : \"== Physical Plan ==\\nAdaptiveSparkPlan (10)\\n+- == Final Plan ==\\n   * HashAggregate (6)\\n   +- ShuffleQueryStage (5), Statistics(sizeInBytes=160.0 B, rowCount=10)\\n      +- Exchange (4)\\n         +- * HashAggregate (3)\\n            +- * ColumnarToRow (2)\\n               +- Scan parquet  (1)\\n+- == Initial Plan ==\\n   HashAggregate (9)\\n   +- Exchange (8)\\n      +- HashAggregate (7)\\n         +- Scan parquet  (1)\\n\\n\\n(1) Scan parquet \\nOutput: []\\nBatched: true\\nLocation: InMemoryFileIndex [file:/Users/ugurkalkavan/tmp/df_str]\\nReadSchema: struct<>\\n\\n(2) ColumnarToRow [codegen id : 1]\\nInput: []\\n\\n(3) HashAggregate [codegen id : 1]\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#94L]\\nResults [1]: [count#95L]\\n\\n(4) Exchange\\nInput [1]: [count#95L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=152]\\n\\n(5) ShuffleQueryStage\\nOutput [1]: [count#95L]\\nArguments: 0\\n\\n(6) HashAggregate [codegen id : 2]\\nInput [1]: [count#95L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#91L]\\nResults [1]: [count(1)#91L AS count#92L]\\n\\n(7) HashAggregate\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#94L]\\nResults [1]: [count#95L]\\n\\n(8) Exchange\\nInput [1]: [count#95L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=140]\\n\\n(9) HashAggregate\\nInput [1]: [count#95L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#91L]\\nResults [1]: [count(1)#91L AS count#92L]\\n\\n(10) AdaptiveSparkPlan\\nOutput [1]: [count#92L]\\nArguments: isFinalPlan=true\\n\\n\",\n",
      "  \"submissionTime\" : \"2024-06-06T13:27:27.328GMT\",\n",
      "  \"duration\" : 634,\n",
      "  \"runningJobIds\" : [ ],\n",
      "  \"successJobIds\" : [ 10, 11 ],\n",
      "  \"failedJobIds\" : [ ],\n",
      "  \"nodes\" : [ {\n",
      "    \"nodeId\" : 7,\n",
      "    \"nodeName\" : \"Scan parquet\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"number of files read\",\n",
      "      \"value\" : \"1\"\n",
      "    }, {\n",
      "      \"name\" : \"scan time\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n742 ms (70 ms, 72 ms, 83 ms (stage 13.0: task 1066))\"\n",
      "    }, {\n",
      "      \"name\" : \"metadata time\",\n",
      "      \"value\" : \"6 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"size of files read\",\n",
      "      \"value\" : \"290.5 MiB\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"100,000,000\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 6,\n",
      "    \"nodeName\" : \"ColumnarToRow\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"100,000,000\"\n",
      "    }, {\n",
      "      \"name\" : \"number of input batches\",\n",
      "      \"value\" : \"24,416\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 5,\n",
      "    \"nodeName\" : \"HashAggregate\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"time in aggregation build\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n915 ms (70 ms, 73 ms, 144 ms (stage 13.0: task 1063))\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"10\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 4,\n",
      "    \"nodeName\" : \"WholeStageCodegen (1)\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"duration\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n989 ms (80 ms, 81 ms, 147 ms (stage 13.0: task 1063))\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 3,\n",
      "    \"nodeName\" : \"Exchange\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"shuffle records written\",\n",
      "      \"value\" : \"10\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged chunks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"shuffle write time\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n42 ms (0 ms, 5 ms, 6 ms (stage 13.0: task 1057))\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged blocks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"corrupt merged block chunks\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged reqs duration\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged blocks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"records read\",\n",
      "      \"value\" : \"10\"\n",
      "    }, {\n",
      "      \"name\" : \"local bytes read\",\n",
      "      \"value\" : \"569.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"fetch wait time\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"merged fetch fallback count\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"local blocks read\",\n",
      "      \"value\" : \"10\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged chunks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"remote blocks read\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"data size\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n160.0 B (0.0 B, 16.0 B, 16.0 B (stage 13.0: task 1065))\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"number of partitions\",\n",
      "      \"value\" : \"1\"\n",
      "    }, {\n",
      "      \"name\" : \"remote reqs duration\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote bytes read to disk\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"shuffle bytes written\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n569.0 B (0.0 B, 56.0 B, 59.0 B (stage 13.0: task 1066))\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 2,\n",
      "    \"nodeName\" : \"HashAggregate\",\n",
      "    \"wholeStageCodegenId\" : 2,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"time in aggregation build\",\n",
      "      \"value\" : \"4 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"1\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 1,\n",
      "    \"nodeName\" : \"WholeStageCodegen (2)\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"duration\",\n",
      "      \"value\" : \"5 ms\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 0,\n",
      "    \"nodeName\" : \"AdaptiveSparkPlan\",\n",
      "    \"metrics\" : [ ]\n",
      "  } ],\n",
      "  \"edges\" : [ {\n",
      "    \"fromId\" : 2,\n",
      "    \"toId\" : 0\n",
      "  }, {\n",
      "    \"fromId\" : 3,\n",
      "    \"toId\" : 2\n",
      "  }, {\n",
      "    \"fromId\" : 5,\n",
      "    \"toId\" : 3\n",
      "  }, {\n",
      "    \"fromId\" : 6,\n",
      "    \"toId\" : 5\n",
      "  }, {\n",
      "    \"fromId\" : 7,\n",
      "    \"toId\" : 6\n",
      "  } ]\n",
      "} ]\n"
     ]
    }
   ],
   "source": [
    "metrics = get_scan_parquet_metrics(app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "7ac89f9b-33b4-40fc-adde-561bc3c4a2ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T13:40:32.195898Z",
     "iopub.status.busy": "2024-06-06T13:40:32.195668Z",
     "iopub.status.idle": "2024-06-06T13:40:32.198255Z",
     "shell.execute_reply": "2024-06-06T13:40:32.197871Z",
     "shell.execute_reply.started": "2024-06-06T13:40:32.195881Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number of files read': '8,796', 'scan time': 'total (min, med, max (stageId: taskId))\\n39.9 s (6 ms, 51 ms, 655 ms (stage 1.0: task 3))', 'dynamic partition pruning time': '0 ms', 'metadata time': '38 ms', 'size of files read': '27.7 GiB', 'number of output rows': '3,604,627,124', 'number of partitions read': '2,932'}\n",
      "Processing SQL_ID: 0\n",
      "Number of files read:             8,796\n",
      "Scan time:                        total (min, med, max (stageId: taskId))\n",
      "39.9 s (6 ms, 51 ms, 655 ms (stage 1.0: task 3))\n",
      "Metadata time:                    38 ms\n",
      "Size of files read:               27.7 GiB\n",
      "Number of output rows:            3,604,627,124\n",
      "Average number of files by partition: 3.0\n",
      "Average file size: 3.2 MiB\n",
      "Tiny file PROBLEM!!!\n",
      "\n",
      "{'number of files read': '8,796', 'scan time': 'total (min, med, max (stageId: taskId))\\n17.3 s (1 ms, 25 ms, 387 ms (stage 5.0: task 501))', 'dynamic partition pruning time': '0 ms', 'metadata time': '11 ms', 'size of files read': '27.7 GiB', 'number of output rows': '3,604,627,124', 'number of partitions read': '2,932'}\n",
      "Processing SQL_ID: 1\n",
      "Number of files read:             8,796\n",
      "Scan time:                        total (min, med, max (stageId: taskId))\n",
      "17.3 s (1 ms, 25 ms, 387 ms (stage 5.0: task 501))\n",
      "Metadata time:                    11 ms\n",
      "Size of files read:               27.7 GiB\n",
      "Number of output rows:            3,604,627,124\n",
      "Average number of files by partition: 3.0\n",
      "Average file size: 3.2 MiB\n",
      "Tiny file PROBLEM!!!\n",
      "\n",
      "{'number of files read': '1,101', 'scan time': 'total (min, med, max (stageId: taskId))\\n1.8 s (8 ms, 28 ms, 58 ms (stage 9.0: task 999))', 'dynamic partition pruning time': '0 ms', 'metadata time': '24 ms', 'size of files read': '3.5 GiB', 'number of output rows': '450,787,193', 'number of partitions read': '367'}\n",
      "Processing SQL_ID: 2\n",
      "Number of files read:             1,101\n",
      "Scan time:                        total (min, med, max (stageId: taskId))\n",
      "1.8 s (8 ms, 28 ms, 58 ms (stage 9.0: task 999))\n",
      "Metadata time:                    24 ms\n",
      "Size of files read:               3.5 GiB\n",
      "Number of output rows:            450,787,193\n",
      "Average number of files by partition: 3.0\n",
      "Average file size: 3.3 MiB\n",
      "Tiny file PROBLEM!!!\n",
      "\n",
      "{'number of files read': '1', 'scan time': 'total (min, med, max (stageId: taskId))\\n742 ms (70 ms, 72 ms, 83 ms (stage 13.0: task 1066))', 'metadata time': '6 ms', 'size of files read': '290.5 MiB', 'number of output rows': '100,000,000'}\n",
      "Processing SQL_ID: 3\n",
      "Number of files read:             1\n",
      "Scan time:                        total (min, med, max (stageId: taskId))\n",
      "742 ms (70 ms, 72 ms, 83 ms (stage 13.0: task 1066))\n",
      "Metadata time:                    6 ms\n",
      "Size of files read:               290.5 MiB\n",
      "Number of output rows:            100,000,000\n",
      "Average number of files by partition: 1.0\n",
      "Average file size: 290.5 MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detect_tiny_files(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352fc30-44a0-48db-8c41-7c490e63d128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1e2a29f0-766e-4d5a-9c2c-85ee21fb587b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T12:38:23.192477Z",
     "iopub.status.busy": "2024-06-06T12:38:23.191912Z",
     "iopub.status.idle": "2024-06-06T12:38:23.198686Z",
     "shell.execute_reply": "2024-06-06T12:38:23.197916Z",
     "shell.execute_reply.started": "2024-06-06T12:38:23.192434Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sql_details(app_id):\n",
    "    try:\n",
    "        response = requests.get(f\"{SPARK_API_ENDPOINT}/{app_id}/sql\")\n",
    "        sql = json.loads(response.text)\n",
    "        #print(response.text)\n",
    "        #print(sql['nodes'])\n",
    "        metrics_dict = get_scan_parquet_metrics(app_id)\n",
    "        return metrics_dict\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Error in fetching Job and Stage details: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d3cbd5ea-a410-4742-b1df-1895d10bfae8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T12:38:23.422195Z",
     "iopub.status.busy": "2024-06-06T12:38:23.421694Z",
     "iopub.status.idle": "2024-06-06T12:38:23.468793Z",
     "shell.execute_reply": "2024-06-06T12:38:23.468422Z",
     "shell.execute_reply.started": "2024-06-06T12:38:23.422164Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number of files read': '1,101',\n",
       " 'scan time': 'total (min, med, max (stageId: taskId))\\n1.8 s (8 ms, 28 ms, 58 ms (stage 9.0: task 999))',\n",
       " 'dynamic partition pruning time': '0 ms',\n",
       " 'metadata time': '24 ms',\n",
       " 'size of files read': '3.5 GiB',\n",
       " 'number of output rows': '450,787,193',\n",
       " 'number of partitions read': '367'}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_details(app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "62767c05-a73e-4707-ab31-7fec28d09611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T11:12:07.611209Z",
     "iopub.status.busy": "2024-06-06T11:12:07.609895Z",
     "iopub.status.idle": "2024-06-06T11:12:07.627803Z",
     "shell.execute_reply": "2024-06-06T11:12:07.627391Z",
     "shell.execute_reply.started": "2024-06-06T11:12:07.611145Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def job_and_stage_details(app_id):\n",
    "    try:\n",
    "            response = requests.get(f\"{SPARK_API_ENDPOINT}/{app_id}/jobs\")\n",
    "            jobs = json.loads(response.text)\n",
    "            \n",
    "            #print(response.text)\n",
    "            \n",
    "            response = requests.get(f\"{SPARK_API_ENDPOINT}/{app_id}/stages/?withSummaries=true&quantiles=0.0,0.25,0.5,0.75,1.0\")\n",
    "            stages = json.loads(response.text)\n",
    "            #print(response.text)\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "         \n",
    "    \n",
    "            #print(stages)\n",
    "            #for stage in stages:\n",
    "            for job in jobs:\n",
    "                job_id = job[\"jobId\"]\n",
    "                job_name = job[\"name\"]\n",
    "                job_stageIds = job[\"stageIds\"]\n",
    "                \n",
    "               \n",
    "\n",
    "\n",
    "                for s_id in job_stageIds:\n",
    "                    for stage in stages:\n",
    "                        if stage['stageId'] == s_id:\n",
    "                            if stage['status'] == 'SKIPPED':\n",
    "                                continue\n",
    "                            \n",
    "                            \n",
    "                            #print(stage[\"taskMetricsDistributions\"])\n",
    "\n",
    "                            # Fetch metrics from the stage dictionary\n",
    "                            input_metrics = stage['taskMetricsDistributions']['inputMetrics']\n",
    "                            output_metrics = stage['taskMetricsDistributions']['outputMetrics']\n",
    "                            shuffle_read_metrics = stage['taskMetricsDistributions']['shuffleReadMetrics']\n",
    "                            shuffle_write_metrics = stage['taskMetricsDistributions']['shuffleWriteMetrics']\n",
    "\n",
    "                            # Assign variables\n",
    "                            bytes_read = input_metrics['bytesRead']\n",
    "                            records_read = input_metrics['recordsRead']\n",
    "                            bytes_written = output_metrics['bytesWritten']\n",
    "                            records_written = output_metrics['recordsWritten']\n",
    "                            \n",
    "                            # Shuffle read metrikleri\n",
    "                            shuffle_read_bytes = shuffle_read_metrics[\"readBytes\"]\n",
    "                            shuffle_read_records = shuffle_read_metrics[\"readRecords\"]\n",
    "\n",
    "                            # Shuffle write metrikleri\n",
    "                            shuffle_write_bytes = shuffle_write_metrics[\"writeBytes\"]\n",
    "                            shuffle_write_records = shuffle_write_metrics[\"writeRecords\"]\n",
    "                            shuffle_write_time = shuffle_write_metrics[\"writeTime\"]\n",
    "\n",
    "\n",
    "                            \n",
    "                            print(f\"Input Bytes Read: {bytes_read}\")\n",
    "                            print(f\"Input Records Read: {records_read}\")\n",
    "                            print(f\"Output Bytes Written: {bytes_written}\")\n",
    "                            print(f\"Output Records Written: {records_written}\")\n",
    "                            # Değişkenlerin yazdırılması\n",
    "                            print(f\"Shuffle Read Bytes: {shuffle_read_bytes}\")\n",
    "                            print(f\"Shuffle Read Records: {shuffle_read_records}\")\n",
    "                            print(f\"Shuffle Write Bytes: {shuffle_write_bytes}\")\n",
    "                            print(f\"Shuffle Write Records: {shuffle_write_records}\")\n",
    "                            print(f\"Shuffle Write Time: {shuffle_write_time}\")\n",
    "                            \n",
    "                            if detect_anomalies(shuffle_read_bytes) or detect_anomalies(shuffle_write_bytes):\n",
    "                                print(\"#### There might be skew!!! ###\")\n",
    "                                \n",
    "                            disk_spill = stage['diskBytesSpilled']\n",
    "                            memory_spill = stage['memoryBytesSpilled']\n",
    "\n",
    "                            if disk_spill > 0 or memory_spill > 0:\n",
    "\n",
    "                                            print()\n",
    "                                            print(f\"Job {job_id} - Job Name {job_name} - Stage {stage['stageId']} - Disk spilled {sizeof_fmt(disk_spill)} bytes.\")\n",
    "                                            print(f\"Job {job_id} - Job Name {job_name} - Stage {stage['stageId']} - Memory spilled {sizeof_fmt(memory_spill)} bytes.\")\n",
    "                            print()\n",
    "                            print(\"-------------------\")\n",
    "                            print()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Error in fetching Job and Stage details: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b00ff686-fcc6-4968-96df-df5c546b1e27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T11:12:08.334396Z",
     "iopub.status.busy": "2024-06-06T11:12:08.333689Z",
     "iopub.status.idle": "2024-06-06T11:12:08.834810Z",
     "shell.execute_reply": "2024-06-06T11:12:08.834426Z",
     "shell.execute_reply.started": "2024-06-06T11:12:08.334346Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ {\n",
      "  \"id\" : 0,\n",
      "  \"status\" : \"COMPLETED\",\n",
      "  \"description\" : \"scan tiny files\",\n",
      "  \"planDescription\" : \"== Physical Plan ==\\nAdaptiveSparkPlan (12)\\n+- == Final Plan ==\\n   * HashAggregate (7)\\n   +- ShuffleQueryStage (6), Statistics(sizeInBytes=7.7 KiB, rowCount=494)\\n      +- Exchange (5)\\n         +- * HashAggregate (4)\\n            +- * Project (3)\\n               +- * ColumnarToRow (2)\\n                  +- Scan parquet  (1)\\n+- == Initial Plan ==\\n   HashAggregate (11)\\n   +- Exchange (10)\\n      +- HashAggregate (9)\\n         +- Project (8)\\n            +- Scan parquet  (1)\\n\\n\\n(1) Scan parquet \\nOutput [3]: [year#5, month#6, day#7]\\nBatched: true\\nLocation: InMemoryFileIndex [file:/Users/ugurkalkavan/Downloads/m06sparkbasics/weather]\\nReadSchema: struct<>\\n\\n(2) ColumnarToRow [codegen id : 1]\\nInput [3]: [year#5, month#6, day#7]\\n\\n(3) Project [codegen id : 1]\\nOutput: []\\nInput [3]: [year#5, month#6, day#7]\\n\\n(4) HashAggregate [codegen id : 1]\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#27L]\\nResults [1]: [count#28L]\\n\\n(5) Exchange\\nInput [1]: [count#28L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=29]\\n\\n(6) ShuffleQueryStage\\nOutput [1]: [count#28L]\\nArguments: 0\\n\\n(7) HashAggregate [codegen id : 2]\\nInput [1]: [count#28L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#24L]\\nResults [1]: [count(1)#24L AS count#25L]\\n\\n(8) Project\\nOutput: []\\nInput [3]: [year#5, month#6, day#7]\\n\\n(9) HashAggregate\\nInput: []\\nKeys: []\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#27L]\\nResults [1]: [count#28L]\\n\\n(10) Exchange\\nInput [1]: [count#28L]\\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=13]\\n\\n(11) HashAggregate\\nInput [1]: [count#28L]\\nKeys: []\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#24L]\\nResults [1]: [count(1)#24L AS count#25L]\\n\\n(12) AdaptiveSparkPlan\\nOutput [1]: [count#25L]\\nArguments: isFinalPlan=true\\n\\n\",\n",
      "  \"submissionTime\" : \"2024-06-06T10:07:44.423GMT\",\n",
      "  \"duration\" : 8446,\n",
      "  \"runningJobIds\" : [ ],\n",
      "  \"successJobIds\" : [ 1, 2 ],\n",
      "  \"failedJobIds\" : [ ],\n",
      "  \"nodes\" : [ {\n",
      "    \"nodeId\" : 8,\n",
      "    \"nodeName\" : \"Scan parquet\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"number of files read\",\n",
      "      \"value\" : \"8,796\"\n",
      "    }, {\n",
      "      \"name\" : \"scan time\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n39.9 s (6 ms, 51 ms, 655 ms (stage 1.0: task 3))\"\n",
      "    }, {\n",
      "      \"name\" : \"dynamic partition pruning time\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"metadata time\",\n",
      "      \"value\" : \"38 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"size of files read\",\n",
      "      \"value\" : \"27.7 GiB\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"3,604,627,124\"\n",
      "    }, {\n",
      "      \"name\" : \"number of partitions read\",\n",
      "      \"value\" : \"2,932\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 7,\n",
      "    \"nodeName\" : \"ColumnarToRow\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"3,604,627,124\"\n",
      "    }, {\n",
      "      \"name\" : \"number of input batches\",\n",
      "      \"value\" : \"882,616\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 6,\n",
      "    \"nodeName\" : \"Project\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ ]\n",
      "  }, {\n",
      "    \"nodeId\" : 5,\n",
      "    \"nodeName\" : \"HashAggregate\",\n",
      "    \"wholeStageCodegenId\" : 1,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"time in aggregation build\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n46.8 s (14 ms, 61 ms, 812 ms (stage 1.0: task 1))\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"494\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 4,\n",
      "    \"nodeName\" : \"WholeStageCodegen (1)\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"duration\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n48.0 s (16 ms, 62 ms, 860 ms (stage 1.0: task 1))\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 3,\n",
      "    \"nodeName\" : \"Exchange\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"shuffle records written\",\n",
      "      \"value\" : \"494\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged chunks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"shuffle write time\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n1.1 s (0 ms, 0 ms, 94 ms (stage 1.0: task 5))\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged blocks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"corrupt merged block chunks\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged reqs duration\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged blocks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"records read\",\n",
      "      \"value\" : \"494\"\n",
      "    }, {\n",
      "      \"name\" : \"local bytes read\",\n",
      "      \"value\" : \"28.5 KiB\"\n",
      "    }, {\n",
      "      \"name\" : \"fetch wait time\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"merged fetch fallback count\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"local blocks read\",\n",
      "      \"value\" : \"494\"\n",
      "    }, {\n",
      "      \"name\" : \"remote merged chunks fetched\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"remote blocks read\",\n",
      "      \"value\" : \"0\"\n",
      "    }, {\n",
      "      \"name\" : \"data size\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n7.7 KiB (0.0 B, 16.0 B, 16.0 B (stage 1.0: task 1))\"\n",
      "    }, {\n",
      "      \"name\" : \"local merged bytes read\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"number of partitions\",\n",
      "      \"value\" : \"1\"\n",
      "    }, {\n",
      "      \"name\" : \"remote reqs duration\",\n",
      "      \"value\" : \"0 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"remote bytes read to disk\",\n",
      "      \"value\" : \"0.0 B\"\n",
      "    }, {\n",
      "      \"name\" : \"shuffle bytes written\",\n",
      "      \"value\" : \"total (min, med, max (stageId: taskId))\\n28.5 KiB (0.0 B, 59.0 B, 59.0 B (stage 1.0: task 1))\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 2,\n",
      "    \"nodeName\" : \"HashAggregate\",\n",
      "    \"wholeStageCodegenId\" : 2,\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"time in aggregation build\",\n",
      "      \"value\" : \"40 ms\"\n",
      "    }, {\n",
      "      \"name\" : \"number of output rows\",\n",
      "      \"value\" : \"1\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 1,\n",
      "    \"nodeName\" : \"WholeStageCodegen (2)\",\n",
      "    \"metrics\" : [ {\n",
      "      \"name\" : \"duration\",\n",
      "      \"value\" : \"41 ms\"\n",
      "    } ]\n",
      "  }, {\n",
      "    \"nodeId\" : 0,\n",
      "    \"nodeName\" : \"AdaptiveSparkPlan\",\n",
      "    \"metrics\" : [ ]\n",
      "  } ],\n",
      "  \"edges\" : [ {\n",
      "    \"fromId\" : 2,\n",
      "    \"toId\" : 0\n",
      "  }, {\n",
      "    \"fromId\" : 3,\n",
      "    \"toId\" : 2\n",
      "  }, {\n",
      "    \"fromId\" : 5,\n",
      "    \"toId\" : 3\n",
      "  }, {\n",
      "    \"fromId\" : 6,\n",
      "    \"toId\" : 5\n",
      "  }, {\n",
      "    \"fromId\" : 7,\n",
      "    \"toId\" : 6\n",
      "  }, {\n",
      "    \"fromId\" : 8,\n",
      "    \"toId\" : 7\n",
      "  } ]\n",
      "} ]\n",
      "Input Bytes Read: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Input Records Read: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Output Bytes Written: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Output Records Written: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Shuffle Read Bytes: [29146.0, 29146.0, 29146.0, 29146.0, 29146.0]\n",
      "Shuffle Read Records: [494.0, 494.0, 494.0, 494.0, 494.0]\n",
      "Shuffle Write Bytes: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Shuffle Write Records: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Shuffle Write Time: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "-------------------\n",
      "\n",
      "Input Bytes Read: [120432.0, 128492.0, 193120.0, 214452.0, 268656.0]\n",
      "Input Records Read: [2633462.0, 4401700.0, 6048380.0, 11572496.0, 13082592.0]\n",
      "Output Bytes Written: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Output Records Written: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Shuffle Read Bytes: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Shuffle Read Records: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Shuffle Write Bytes: [59.0, 59.0, 59.0, 59.0, 59.0]\n",
      "Shuffle Write Records: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Shuffle Write Time: [206540.0, 334500.0, 682668.0, 1735125.0, 94804084.0]\n",
      "\n",
      "-------------------\n",
      "\n",
      "Input Bytes Read: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Input Records Read: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Output Bytes Written: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Output Records Written: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Shuffle Read Bytes: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Shuffle Read Records: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Shuffle Write Bytes: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Shuffle Write Records: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Shuffle Write Time: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app_id = fetch_spark_apps(spark)\n",
    "job_and_stage_details(app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01d66a02-df53-4893-b276-18bfd2ae2d59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:47:34.731757Z",
     "iopub.status.busy": "2024-06-06T10:47:34.731203Z",
     "iopub.status.idle": "2024-06-06T10:47:34.744762Z",
     "shell.execute_reply": "2024-06-06T10:47:34.743763Z",
     "shell.execute_reply.started": "2024-06-06T10:47:34.731710Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies: True\n"
     ]
    }
   ],
   "source": [
    "shuffle_write_records = [1.0, 1.0, 400.0, 400.0, 400.0]\n",
    "anomalies = detect_anomalies(shuffle_write_records)\n",
    "\n",
    "print(\"Anomalies:\", anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae79b2-f623-4d44-9d18-191a36208916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3375bed-e3ab-4f15-989f-f42c2ce6788b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:11:04.642454Z",
     "iopub.status.busy": "2024-06-06T10:11:04.641360Z",
     "iopub.status.idle": "2024-06-06T10:11:04.651685Z",
     "shell.execute_reply": "2024-06-06T10:11:04.650677Z",
     "shell.execute_reply.started": "2024-06-06T10:11:04.642404Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 10 haneli sayılardan oluşan örnek bir liste\n",
    "data = [4009375.0, 5461460.0, 90484129.0, 157407576.0, 173781832.0] #TRUE\n",
    "data2 = [190378691.0, 191422987.0, 192053189.0, 199053189.0, 199953189.0] #FALSE\n",
    "data3 = [5150254.0, 5151575.0, 5154254.0, 5155274.0, 5314861.0] #FALSE\n",
    "data4 = [58.0, 58.0, 58.0, 58.0, 59.0] #FALSE\n",
    "data5 = [4009375.0, 5461460.0, 16484129.0, 157407576.0, 173781832.0] #TRUE\n",
    "data6 = [1.0, 1.0, 1.0, 1.0, 1.0] #FALSE\n",
    "data6 = [0.0, 0.0, 0.0, 1.0, 1.0] #FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be64d6f-eb3a-4052-8237-c8e9ff4f94f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:10:43.761429Z",
     "iopub.status.busy": "2024-06-06T10:10:43.760634Z",
     "iopub.status.idle": "2024-06-06T10:10:43.786180Z",
     "shell.execute_reply": "2024-06-06T10:10:43.785470Z",
     "shell.execute_reply.started": "2024-06-06T10:10:43.761376Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect_anomalies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m data6 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m] \u001b[38;5;66;03m#FALSE\u001b[39;00m\n\u001b[1;32m      8\u001b[0m data6 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m] \u001b[38;5;66;03m#FALSE\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(detect_anomalies(data))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(detect_anomalies(data2))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(detect_anomalies(data3))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'detect_anomalies' is not defined"
     ]
    }
   ],
   "source": [
    "print(detect_anomalies(data))\n",
    "print(detect_anomalies(data2))\n",
    "print(detect_anomalies(data3))\n",
    "print(detect_anomalies(data4))\n",
    "print(detect_anomalies(data5))\n",
    "print(detect_anomalies(data6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "435b495c-7e7d-4ff6-8bc8-321f5c1fa7c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:13:05.481720Z",
     "iopub.status.busy": "2024-06-06T10:13:05.481132Z",
     "iopub.status.idle": "2024-06-06T10:13:05.486980Z",
     "shell.execute_reply": "2024-06-06T10:13:05.486149Z",
     "shell.execute_reply.started": "2024-06-06T10:13:05.481693Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8 MiB'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizeof_fmt(4009375.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83923f59-9bb6-4717-9db9-c568ba1f8e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:13:28.242925Z",
     "iopub.status.busy": "2024-06-06T10:13:28.242224Z",
     "iopub.status.idle": "2024-06-06T10:13:28.249926Z",
     "shell.execute_reply": "2024-06-06T10:13:28.248753Z",
     "shell.execute_reply.started": "2024-06-06T10:13:28.242880Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'86.3 MiB'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizeof_fmt(90484129.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2dfa9749-c685-4fc0-8ec5-fdac2493b4ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:14:54.708024Z",
     "iopub.status.busy": "2024-06-06T10:14:54.706611Z",
     "iopub.status.idle": "2024-06-06T10:14:54.715257Z",
     "shell.execute_reply": "2024-06-06T10:14:54.714715Z",
     "shell.execute_reply.started": "2024-06-06T10:14:54.707963Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'165.7 MiB'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizeof_fmt(173781832.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02c08953-4c59-4473-a5fe-44885a267705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:11:22.511356Z",
     "iopub.status.busy": "2024-06-06T10:11:22.510832Z",
     "iopub.status.idle": "2024-06-06T10:11:22.515761Z",
     "shell.execute_reply": "2024-06-06T10:11:22.514705Z",
     "shell.execute_reply.started": "2024-06-06T10:11:22.511317Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52c27828-cded-430b-8633-55c83b25a525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:11:35.097536Z",
     "iopub.status.busy": "2024-06-06T10:11:35.097008Z",
     "iopub.status.idle": "2024-06-06T10:11:35.104030Z",
     "shell.execute_reply": "2024-06-06T10:11:35.102912Z",
     "shell.execute_reply.started": "2024-06-06T10:11:35.097508Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86228874.4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed749e44-7529-4ed6-bb6c-677eaf866dab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:11:44.130914Z",
     "iopub.status.busy": "2024-06-06T10:11:44.130217Z",
     "iopub.status.idle": "2024-06-06T10:11:44.141274Z",
     "shell.execute_reply": "2024-06-06T10:11:44.140433Z",
     "shell.execute_reply.started": "2024-06-06T10:11:44.130867Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90484129.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "506fa663-d1d0-443f-8ba3-fdc2a6439b0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:13:39.954818Z",
     "iopub.status.busy": "2024-06-06T10:13:39.954277Z",
     "iopub.status.idle": "2024-06-06T10:13:39.962057Z",
     "shell.execute_reply": "2024-06-06T10:13:39.961400Z",
     "shell.execute_reply.started": "2024-06-06T10:13:39.954778Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4255254.599999994"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(data) - np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b81b7-1110-4c9e-a995-d5e3ff060852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c23da-81f4-4261-8fc5-b63d41d8bb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a70ec9-c0ea-455c-9e22-2d271a646f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1463,
   "id": "78989ad2-31f8-470d-ba83-038d75022d61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T09:59:32.263227Z",
     "iopub.status.busy": "2024-06-06T09:59:32.262612Z",
     "iopub.status.idle": "2024-06-06T09:59:32.282960Z",
     "shell.execute_reply": "2024-06-06T09:59:32.282512Z",
     "shell.execute_reply.started": "2024-06-06T09:59:32.263181Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def predict_num_partitions(files):\n",
    "    ## What is the maximum size of each spark-partition (default value)?\n",
    "    defaultMaxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\",\"\"))\n",
    "\n",
    "    ## What is the cost in bytes for each file (default value)?\n",
    "    open_cost_bytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\").replace(\"b\",\"\"))\n",
    "    \n",
    "    max_partition_bytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\",\"\"))\n",
    "\n",
    "    \n",
    "    #actual_bytes = sum([file.size for file in files])               # Total size of the dataset on disk \n",
    "    actual_bytes = sum([os.path.getsize(file_path) for file_path in files])\n",
    "    padded_bytes = actual_bytes + (len(files) * open_cost_bytes)          # Final size with padding from openCost\n",
    "\n",
    "    bytes_per_core = (padded_bytes/spark.sparkContext.defaultParallelism)           # The number of bytes per core\n",
    "    max_of_cost_BPC = max(open_cost_bytes, bytes_per_core)                # Larger of openCost and bytesPerCore\n",
    "    target_size = min(max_partition_bytes , max_of_cost_BPC)        # Smaller of maxPartitionBytes and maxOfCostBPC\n",
    "    partitions = padded_bytes /  float(target_size)                 # The final number of partitions (needs to be rounded up)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"defaultMaxPartitionBytes:\", defaultMaxPartitionBytes)\n",
    "    print(\"\")\n",
    "    print(\"---\")\n",
    "    print(\"File Count:\", len(files))\n",
    "    print(\"Actual Bytes:\", actual_bytes)\n",
    "    print(\"Padded Bytes:\", padded_bytes, \"Actual_Bytes + (File_Count * Open_Cost)\")\n",
    "    print(\"Average Size:\", (padded_bytes/len(files)))\n",
    "    print(\"---\")\n",
    "    print(\"Open Cost:\", open_cost_bytes, \"spark.sql.files.openCostInBytes\")\n",
    "    print(\"Bytes-Per-Core:\", bytes_per_core, \"padded_bytes / Default Parallelism\")\n",
    "    print(\"Max Cost:\", max_of_cost_BPC, \"(max of Open_Cost & Bytes-Per-Core)\")\n",
    "    print(\"---\")\n",
    "    print(\"Max Partition Bytes:\", max_partition_bytes, \"spark.sql.files.maxPartitionBytes\")\n",
    "    print(\"Target Size:\", target_size, \"(min of Max_Cost & Max_Partition_Bytes)\")\n",
    "    print(\"---\")\n",
    "    print(\"Number of Partions:\", math.ceil(partitions), f\"({partitions} from Padded_Bytes / Target_Size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca71fb0-2c45-43c2-8c8f-226f461fea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_num_files(df: Dataframe) -> int:\n",
    "    ## What is the maximum size of each spark-partition (default value)?\n",
    "    defaultMaxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\",\"\"))\n",
    "\n",
    "    ## What is the cost in bytes for each file (default value)?\n",
    "    open_cost_bytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\").replace(\"b\",\"\"))\n",
    "    \n",
    "    max_partition_bytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\",\"\"))\n",
    "\n",
    "    \n",
    "    #actual_bytes = sum([file.size for file in files])               # Total size of the dataset on disk \n",
    "    actual_bytes = sum([os.path.getsize(file_path) for file_path in files])\n",
    "    padded_bytes = actual_bytes + (len(files) * open_cost_bytes)          # Final size with padding from openCost\n",
    "\n",
    "    bytes_per_core = (padded_bytes/spark.sparkContext.defaultParallelism)           # The number of bytes per core\n",
    "    max_of_cost_BPC = max(open_cost_bytes, bytes_per_core)                # Larger of openCost and bytesPerCore\n",
    "    target_size = min(max_partition_bytes , max_of_cost_BPC)        # Smaller of maxPartitionBytes and maxOfCostBPC\n",
    "    partitions = padded_bytes /  float(target_size)                 # The final number of partitions (needs to be rounded up)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"defaultMaxPartitionBytes:\", defaultMaxPartitionBytes)\n",
    "    print(\"\")\n",
    "    print(\"---\")\n",
    "    print(\"File Count:\", len(files))\n",
    "    print(\"Actual Bytes:\", actual_bytes)\n",
    "    print(\"Padded Bytes:\", padded_bytes, \"Actual_Bytes + (File_Count * Open_Cost)\")\n",
    "    print(\"Average Size:\", (padded_bytes/len(files)))\n",
    "    print(\"---\")\n",
    "    print(\"Open Cost:\", open_cost_bytes, \"spark.sql.files.openCostInBytes\")\n",
    "    print(\"Bytes-Per-Core:\", bytes_per_core, \"padded_bytes / Default Parallelism\")\n",
    "    print(\"Max Cost:\", max_of_cost_BPC, \"(max of Open_Cost & Bytes-Per-Core)\")\n",
    "    print(\"---\")\n",
    "    print(\"Max Partition Bytes:\", max_partition_bytes, \"spark.sql.files.maxPartitionBytes\")\n",
    "    print(\"Target Size:\", target_size, \"(min of Max_Cost & Max_Partition_Bytes)\")\n",
    "    print(\"---\")\n",
    "    print(\"Number of Partions:\", math.ceil(partitions), f\"({partitions} from Padded_Bytes / Target_Size)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
