{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa66e81d-a69c-4fda-94f5-fb708ee17871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:03:00.683244Z",
     "iopub.status.busy": "2024-06-03T18:03:00.683115Z",
     "iopub.status.idle": "2024-06-03T18:03:04.883543Z",
     "shell.execute_reply": "2024-06-03T18:03:04.883230Z",
     "shell.execute_reply.started": "2024-06-03T18:03:00.683231Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 21:03:02 WARN Utils: Your hostname, Ugurs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.57 instead (on interface en0)\n",
      "24/06/03 21:03:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/03 21:03:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('casting').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11fde08-0e7d-4912-94b8-c567c56be1c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:03:04.887482Z",
     "iopub.status.busy": "2024-06-03T18:03:04.887423Z",
     "iopub.status.idle": "2024-06-03T18:03:04.889241Z",
     "shell.execute_reply": "2024-06-03T18:03:04.889027Z",
     "shell.execute_reply.started": "2024-06-03T18:03:04.887473Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10b1fbc1-9db5-4506-b939-3fed9092a0e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:04:02.108093Z",
     "iopub.status.busy": "2024-06-03T18:04:02.107065Z",
     "iopub.status.idle": "2024-06-03T18:04:02.115704Z",
     "shell.execute_reply": "2024-06-03T18:04:02.114452Z",
     "shell.execute_reply.started": "2024-06-03T18:04:02.108048Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_int_path = \"/Users/ugurkalkavan/tmp/df_int\"\n",
    "df_int_sorted_path = \"/Users/ugurkalkavan/tmp/df_int_sorted\"\n",
    "df_str_path = \"/Users/ugurkalkavan/tmp/df_str\"\n",
    "df_str_sorted_path = \"/Users/ugurkalkavan/tmp/df_str_sorted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b7aae36-461b-483c-b728-935a3e1d0ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:04:04.907666Z",
     "iopub.status.busy": "2024-06-03T18:04:04.907088Z",
     "iopub.status.idle": "2024-06-03T18:06:48.748157Z",
     "shell.execute_reply": "2024-06-03T18:06:48.747501Z",
     "shell.execute_reply.started": "2024-06-03T18:04:04.907630Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create a DataFrame with 10k rows of random numbers\n",
    "df_int = spark.range(0, 100000000).select((rand()*10000000000).cast('int').alias('age'))\n",
    "\n",
    "\n",
    "\n",
    "df_int.repartition(1).write.mode('overwrite').parquet(df_int_path)\n",
    "\n",
    "df_int_sorted = df_int.sort(\"age\")\n",
    "\n",
    "\n",
    "\n",
    "df_int_sorted.repartition(1).write.mode('overwrite').parquet(df_int_sorted_path)\n",
    "\n",
    "# Create a DataFrame with 10k rows of random numbers but cast to string\n",
    "df_str = df_int.select(col('age').cast('string'))\n",
    "\n",
    "\n",
    "\n",
    "df_str.repartition(1).write.mode('overwrite').parquet(df_str_path)\n",
    "\n",
    "df_str_sorted = df_str.sort(\"age\")\n",
    "\n",
    "df_str_sorted.repartition(1).write.mode('overwrite').parquet(df_str_sorted_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f08e468-c74d-4053-94dc-c2fd064be035",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:09:24.102316Z",
     "iopub.status.busy": "2024-06-03T18:09:24.101120Z",
     "iopub.status.idle": "2024-06-03T18:09:24.120999Z",
     "shell.execute_reply": "2024-06-03T18:09:24.120257Z",
     "shell.execute_reply.started": "2024-06-03T18:09:24.102248Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set (\"spark.sql.files.maxPartitionBytes\", \"134217728b\") #10000b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd3be4fe-8a4c-473a-ae1d-bc9c8e49913c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:09:45.677703Z",
     "iopub.status.busy": "2024-06-03T18:09:45.676188Z",
     "iopub.status.idle": "2024-06-03T18:09:50.615027Z",
     "shell.execute_reply": "2024-06-03T18:09:50.614471Z",
     "shell.execute_reply.started": "2024-06-03T18:09:45.677640Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of rdd partitions: 10\n",
      "Time taken with integer from parquet: 0.9 seconds\n",
      "Nr. of rdd partitions: 10\n",
      "Time taken with integer sorted from parquet: 0.31 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of rdd partitions: 10\n",
      "Time taken with string from parquet: 3.19 seconds\n",
      "Nr. of rdd partitions: 10\n",
      "Time taken with string sorted from parquet: 0.51 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "spark.sparkContext.setJobDescription('df_int')  # Setting Job description\n",
    "\n",
    "# Read the integer DataFrame from Parquet file\n",
    "start_time = time.time()\n",
    "\n",
    "df_int = spark.read.parquet(df_int_path)\n",
    "\n",
    "# Perform operation on integer DataFrame\n",
    "df_int.filter(df_int.age < 10008).count()\n",
    "print(\"Nr. of rdd partitions:\", df_int.rdd.getNumPartitions())\n",
    "# End timer and print time\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken with integer from parquet: {round(end_time - start_time, 2)} seconds\")\n",
    "\n",
    "######\n",
    "spark.sparkContext.setJobDescription('df_int_sorted')  # Setting Job description\n",
    "# Read the integer DataFrame from Parquet file\n",
    "start_time = time.time()\n",
    "\n",
    "df_int_sorted = spark.read.parquet(df_int_sorted_path)\n",
    "\n",
    "# Perform operation on integer DataFrame\n",
    "df_int_sorted.filter(df_int_sorted.age < 10008).count()\n",
    "print(\"Nr. of rdd partitions:\", df_int_sorted.rdd.getNumPartitions())\n",
    "# End timer and print time\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken with integer sorted from parquet: {round(end_time - start_time, 2)} seconds\")\n",
    "\n",
    "#####\n",
    "spark.sparkContext.setJobDescription('df_str')  # Setting Job description\n",
    "\n",
    "# Read the string DataFrame from Parquet file\n",
    "start_time = time.time()\n",
    "\n",
    "df_str = spark.read.parquet(df_str_path)\n",
    "\n",
    "# Perform operation on string DataFrame\n",
    "df_str.filter(df_str.age < \"10008\").count()\n",
    "print(\"Nr. of rdd partitions:\", df_str.rdd.getNumPartitions())\n",
    "# End timer and print time\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken with string from parquet: {round(end_time - start_time, 2)} seconds\")\n",
    "\n",
    "######\n",
    "spark.sparkContext.setJobDescription('df_str_sorted')  # Setting Job description\n",
    "\n",
    "# Read the integer DataFrame from Parquet file\n",
    "start_time = time.time()\n",
    "\n",
    "df_str_sorted = spark.read.parquet(df_str_sorted_path)\n",
    "\n",
    "# Perform operation on integer DataFrame\n",
    "df_str_sorted.filter(df_str_sorted.age < \"10008\").count()\n",
    "print(\"Nr. of rdd partitions:\", df_str_sorted.rdd.getNumPartitions())\n",
    "\n",
    "# End timer and print time\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken with string sorted from parquet: {round(end_time - start_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23a33c2a-8241-48d2-ad62-af247eafb2da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:06:56.098307Z",
     "iopub.status.busy": "2024-06-03T18:06:56.098243Z",
     "iopub.status.idle": "2024-06-03T18:06:56.135414Z",
     "shell.execute_reply": "2024-06-03T18:06:56.135049Z",
     "shell.execute_reply.started": "2024-06-03T18:06:56.098301Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.57:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>casting</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x107eecb10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38b59072-f72c-4e85-961d-989bccdaae90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:11:02.373043Z",
     "iopub.status.busy": "2024-06-03T18:11:02.372055Z",
     "iopub.status.idle": "2024-06-03T18:11:02.463853Z",
     "shell.execute_reply": "2024-06-03T18:11:02.463505Z",
     "shell.execute_reply.started": "2024-06-03T18:11:02.372999Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.7 (main, Dec 15 2023, 12:09:56) [Clang 14.0.6 ]\n",
      "pa.__version__='14.0.2'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "print(f\"{sys.version}\")\n",
    "print(f\"{pa.__version__=}\")\n",
    "par_str_path = f\"{df_str_path}/part-00000-5cd38d6b-3135-40e8-8faa-64389b5e5a18-c000.snappy.parquet\"\n",
    "par_int_path = f\"{df_int_path}/part-00000-dd315e0b-beac-43f8-8f2b-1563c3bb7676-c000.snappy.parquet\"\n",
    "par_int_sorted_path = f\"{df_int_sorted_path}/part-00000-853084f4-70f9-4923-b34d-f7c746d35cc0-c000.snappy.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccb5491a-af31-4a14-bfdd-f5088abd4929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:11:05.040404Z",
     "iopub.status.busy": "2024-06-03T18:11:05.038834Z",
     "iopub.status.idle": "2024-06-03T18:11:05.067295Z",
     "shell.execute_reply": "2024-06-03T18:11:05.066746Z",
     "shell.execute_reply.started": "2024-06-03T18:11:05.040332Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq_file_int = pq.ParquetFile(par_int_path)\n",
    "pq_file_str = pq.ParquetFile(par_str_path)\n",
    "pq_file_int_sorted = pq.ParquetFile(par_int_sorted_path)\n",
    "# Get metadata for the i-th RowGroup\n",
    "rg_meta_int = pq_file_int.metadata.row_group(0);\n",
    "rg_meta_str = pq_file_str.metadata.row_group(0);\n",
    "rg_meta_int_sorted = pq_file_int_sorted.metadata.row_group(0);\n",
    "# Get the \"max\" statistic for the k-th column\n",
    "rg_meta_int = rg_meta_int.column(0)\n",
    "rg_meta_str = rg_meta_str.column(0)\n",
    "rg_meta_int_sorted = rg_meta_int_sorted.column(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a00f5f7e-6d94-430f-acca-e9a6b4002b1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:11:06.056832Z",
     "iopub.status.busy": "2024-06-03T18:11:06.056109Z",
     "iopub.status.idle": "2024-06-03T18:11:06.061286Z",
     "shell.execute_reply": "2024-06-03T18:11:06.060568Z",
     "shell.execute_reply.started": "2024-06-03T18:11:06.056787Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_compression_ratio(uncompressed_size, compressed_size):\n",
    "    ratio = uncompressed_size / compressed_size\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9420b6b0-7b9c-4d22-83cd-0a734ba2f2e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:11:57.579484Z",
     "iopub.status.busy": "2024-06-03T18:11:57.577943Z",
     "iopub.status.idle": "2024-06-03T18:11:57.606894Z",
     "shell.execute_reply": "2024-06-03T18:11:57.606140Z",
     "shell.execute_reply.started": "2024-06-03T18:11:57.579417Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.ColumnChunkMetaData object at 0x161f3bd80>\n",
      "  file_offset: 719180\n",
      "  file_path: \n",
      "  physical_type: BYTE_ARRAY\n",
      "  num_values: 44291537\n",
      "  path_in_schema: age\n",
      "  is_stats_set: True\n",
      "  statistics:\n",
      "    <pyarrow._parquet.Statistics object at 0x161f8acf0>\n",
      "      has_min_max: True\n",
      "      min: 1000000266\n",
      "      max: 999999896\n",
      "      null_count: 0\n",
      "      distinct_count: None\n",
      "      num_values: 44291537\n",
      "      physical_type: BYTE_ARRAY\n",
      "      logical_type: String\n",
      "      converted_type (legacy): UTF8\n",
      "  compression: SNAPPY\n",
      "  encodings: ('BIT_PACKED', 'PLAIN_DICTIONARY', 'RLE', 'PLAIN')\n",
      "  has_dictionary_page: True\n",
      "  dictionary_page_offset: 4\n",
      "  data_page_offset: 719180\n",
      "  total_compressed_size: 134821291\n",
      "  total_uncompressed_size: 611882556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.538471271573864"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rg_meta_str)\n",
    "get_compression_ratio(rg_meta_str.total_uncompressed_size,rg_meta_str.total_compressed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "080f2392-53f4-492e-b321-4002db3a73e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:11:58.153435Z",
     "iopub.status.busy": "2024-06-03T18:11:58.152835Z",
     "iopub.status.idle": "2024-06-03T18:11:58.161109Z",
     "shell.execute_reply": "2024-06-03T18:11:58.160325Z",
     "shell.execute_reply.started": "2024-06-03T18:11:58.153388Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.ColumnChunkMetaData object at 0x161f39210>\n",
      "  file_offset: 1046176\n",
      "  file_path: \n",
      "  physical_type: INT32\n",
      "  num_values: 84836597\n",
      "  path_in_schema: age\n",
      "  is_stats_set: True\n",
      "  statistics:\n",
      "    <pyarrow._parquet.Statistics object at 0x161fe0950>\n",
      "      has_min_max: True\n",
      "      min: 285\n",
      "      max: 2147483647\n",
      "      null_count: 0\n",
      "      distinct_count: None\n",
      "      num_values: 84836597\n",
      "      physical_type: INT32\n",
      "      logical_type: None\n",
      "      converted_type (legacy): NONE\n",
      "  compression: SNAPPY\n",
      "  encodings: ('BIT_PACKED', 'PLAIN_DICTIONARY', 'RLE', 'PLAIN')\n",
      "  has_dictionary_page: True\n",
      "  dictionary_page_offset: 4\n",
      "  data_page_offset: 1046176\n",
      "  total_compressed_size: 135239312\n",
      "  total_uncompressed_size: 337905926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4985776768814087"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rg_meta_int)\n",
    "get_compression_ratio(rg_meta_int.total_uncompressed_size,rg_meta_int.total_compressed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af3a0bc5-3a3e-421d-8d83-8d5df732edd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T18:11:58.516068Z",
     "iopub.status.busy": "2024-06-03T18:11:58.515691Z",
     "iopub.status.idle": "2024-06-03T18:11:58.522542Z",
     "shell.execute_reply": "2024-06-03T18:11:58.521842Z",
     "shell.execute_reply.started": "2024-06-03T18:11:58.516046Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.ColumnChunkMetaData object at 0x1476610d0>\n",
      "  file_offset: 4\n",
      "  file_path: \n",
      "  physical_type: INT32\n",
      "  num_values: 100000000\n",
      "  path_in_schema: age\n",
      "  is_stats_set: True\n",
      "  statistics:\n",
      "    <pyarrow._parquet.Statistics object at 0x161fe1e40>\n",
      "      has_min_max: True\n",
      "      min: 28\n",
      "      max: 2147483647\n",
      "      null_count: 0\n",
      "      distinct_count: None\n",
      "      num_values: 100000000\n",
      "      physical_type: INT32\n",
      "      logical_type: None\n",
      "      converted_type (legacy): NONE\n",
      "  compression: SNAPPY\n",
      "  encodings: ('BIT_PACKED', 'RLE', 'PLAIN')\n",
      "  has_dictionary_page: False\n",
      "  dictionary_page_offset: None\n",
      "  data_page_offset: 4\n",
      "  total_compressed_size: 100874685\n",
      "  total_uncompressed_size: 400180992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.967110202128512"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rg_meta_int_sorted)\n",
    "get_compression_ratio(rg_meta_int_sorted.total_uncompressed_size,rg_meta_int_sorted.total_compressed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44bdfd-bc6a-4d96-a1c7-34c655bae287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b1bcb564-ba85-4419-a0e3-814c26a27141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T17:36:25.719950Z",
     "iopub.status.busy": "2024-06-03T17:36:25.718846Z",
     "iopub.status.idle": "2024-06-03T17:36:25.760444Z",
     "shell.execute_reply": "2024-06-03T17:36:25.760094Z",
     "shell.execute_reply.started": "2024-06-03T17:36:25.719894Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.57:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>casting</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10bbaee50>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb4b60-19e1-409c-85ce-96e2a2326d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
