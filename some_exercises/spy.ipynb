{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa54898-de13-4b29-bacc-ba9b54a9f06f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:31.429329Z",
     "iopub.status.busy": "2024-05-30T11:23:31.429218Z",
     "iopub.status.idle": "2024-05-30T11:23:35.611932Z",
     "shell.execute_reply": "2024-05-30T11:23:35.611579Z",
     "shell.execute_reply.started": "2024-05-30T11:23:31.429318Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/30 14:23:33 WARN Utils: Your hostname, Ugurs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.30 instead (on interface en0)\n",
      "24/05/30 14:23:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/30 14:23:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x10850c150>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Stop the existing SparkContext\n",
    "SparkContext.getOrCreate().stop()\n",
    "\n",
    "# Create a Spark configuration object\n",
    "conf = SparkConf()\n",
    "\n",
    "# Set the default parallelism\n",
    "# conf.set(\"spark.default.parallelism\", \"200\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6c6b0c-6ed0-47fc-a0fc-a1c4e1b5625b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:35.612691Z",
     "iopub.status.busy": "2024-05-30T11:23:35.612526Z",
     "iopub.status.idle": "2024-05-30T11:23:35.905090Z",
     "shell.execute_reply": "2024-05-30T11:23:35.904711Z",
     "shell.execute_reply.started": "2024-05-30T11:23:35.612682Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pySparkSetup').config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "777cfa54-8e79-41ae-b7df-4edc18eca56e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:36.092370Z",
     "iopub.status.busy": "2024-05-30T11:23:36.092253Z",
     "iopub.status.idle": "2024-05-30T11:23:36.886759Z",
     "shell.execute_reply": "2024-05-30T11:23:36.886384Z",
     "shell.execute_reply.started": "2024-05-30T11:23:36.092359Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.adaptive.enabled', 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157d7cb0-9260-4556-95d3-1b439e69a26c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:36.889906Z",
     "iopub.status.busy": "2024-05-30T11:23:36.889848Z",
     "iopub.status.idle": "2024-05-30T11:23:36.891527Z",
     "shell.execute_reply": "2024-05-30T11:23:36.891237Z",
     "shell.execute_reply.started": "2024-05-30T11:23:36.889900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parquet_path= \"/Users/ugurkalkavan/Downloads/m06sparkbasics/weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95c480e1-d7ed-4c68-a525-a5c9556524fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:36.891968Z",
     "iopub.status.busy": "2024-05-30T11:23:36.891845Z",
     "iopub.status.idle": "2024-05-30T11:23:36.894474Z",
     "shell.execute_reply": "2024-05-30T11:23:36.894156Z",
     "shell.execute_reply.started": "2024-05-30T11:23:36.891960Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription('read parquet first')  # Setting Job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9498536a-6e2a-4b64-acf0-5b20032765f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:36.895038Z",
     "iopub.status.busy": "2024-05-30T11:23:36.894918Z",
     "iopub.status.idle": "2024-05-30T11:23:40.093156Z",
     "shell.execute_reply": "2024-05-30T11:23:40.092871Z",
     "shell.execute_reply.started": "2024-05-30T11:23:36.895031Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "weather = spark.read.parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc9b97c-f104-42fc-9bd9-addb9e936ce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:40.093711Z",
     "iopub.status.busy": "2024-05-30T11:23:40.093633Z",
     "iopub.status.idle": "2024-05-30T11:23:40.095349Z",
     "shell.execute_reply": "2024-05-30T11:23:40.095072Z",
     "shell.execute_reply.started": "2024-05-30T11:23:40.093704Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4172b951-e7b0-421f-a87b-d1e4809410c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:40.097497Z",
     "iopub.status.busy": "2024-05-30T11:23:40.097418Z",
     "iopub.status.idle": "2024-05-30T11:23:40.100165Z",
     "shell.execute_reply": "2024-05-30T11:23:40.099965Z",
     "shell.execute_reply.started": "2024-05-30T11:23:40.097491Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription('approx_count_distinct')  # Setting Job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58104d72-207a-40a9-a636-c6f4ecfc5783",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:41.165867Z",
     "iopub.status.busy": "2024-05-30T11:23:41.165327Z",
     "iopub.status.idle": "2024-05-30T11:23:50.931445Z",
     "shell.execute_reply": "2024-05-30T11:23:50.931092Z",
     "shell.execute_reply.started": "2024-05-30T11:23:41.165825Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/30 14:23:41 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|approx_count_distinct(lng)|\n",
      "+--------------------------+\n",
      "|                    324657|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.agg(approx_count_distinct(col(\"lng\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fb378a8-9bec-46ac-9a38-311d4c9fe0c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:50.940527Z",
     "iopub.status.busy": "2024-05-30T11:23:50.940440Z",
     "iopub.status.idle": "2024-05-30T11:23:50.943877Z",
     "shell.execute_reply": "2024-05-30T11:23:50.943540Z",
     "shell.execute_reply.started": "2024-05-30T11:23:50.940519Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription('count')  # Setting Job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "720f875d-2e56-4dce-bd48-f365085e1a54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:50.945045Z",
     "iopub.status.busy": "2024-05-30T11:23:50.944941Z",
     "iopub.status.idle": "2024-05-30T11:23:52.063664Z",
     "shell.execute_reply": "2024-05-30T11:23:52.063332Z",
     "shell.execute_reply.started": "2024-05-30T11:23:50.945034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "112394743"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.select(col(\"lng\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "889765ea-bbf1-4e81-9fc4-4fc25ce6d177",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:52.064245Z",
     "iopub.status.busy": "2024-05-30T11:23:52.064169Z",
     "iopub.status.idle": "2024-05-30T11:23:52.067667Z",
     "shell.execute_reply": "2024-05-30T11:23:52.067345Z",
     "shell.execute_reply.started": "2024-05-30T11:23:52.064238Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription('distinct')  # Setting Job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e482acc-21a6-4ed2-91e1-04b38f64bf1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:52.068093Z",
     "iopub.status.busy": "2024-05-30T11:23:52.068023Z",
     "iopub.status.idle": "2024-05-30T11:23:58.306093Z",
     "shell.execute_reply": "2024-05-30T11:23:58.305675Z",
     "shell.execute_reply.started": "2024-05-30T11:23:52.068086Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "339474"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.select(col(\"lng\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dab78d94-dd36-48ea-ac01-ca34bb3aaf25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:26:15.969506Z",
     "iopub.status.busy": "2024-05-30T11:26:15.968576Z",
     "iopub.status.idle": "2024-05-30T11:26:15.981426Z",
     "shell.execute_reply": "2024-05-30T11:26:15.980802Z",
     "shell.execute_reply.started": "2024-05-30T11:26:15.969455Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription('count all')  # Setting Job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa2bc35e-6087-4dd6-b69d-db97dc181884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:26:16.659471Z",
     "iopub.status.busy": "2024-05-30T11:26:16.658773Z",
     "iopub.status.idle": "2024-05-30T11:26:17.299606Z",
     "shell.execute_reply": "2024-05-30T11:26:17.299181Z",
     "shell.execute_reply.started": "2024-05-30T11:26:16.659419Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112394743"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7af9aeb-ab85-4f1b-8f9f-55f13e6c6b73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:58.321940Z",
     "iopub.status.busy": "2024-05-30T11:23:58.321849Z",
     "iopub.status.idle": "2024-05-30T11:23:58.328834Z",
     "shell.execute_reply": "2024-05-30T11:23:58.328559Z",
     "shell.execute_reply.started": "2024-05-30T11:23:58.321932Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.30:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1097f3110>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b22ff0f5-c82b-406d-95c8-6ffc3cd586bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T11:23:58.329452Z",
     "iopub.status.busy": "2024-05-30T11:23:58.329255Z",
     "iopub.status.idle": "2024-05-30T11:23:58.484327Z",
     "shell.execute_reply": "2024-05-30T11:23:58.483987Z",
     "shell.execute_reply.started": "2024-05-30T11:23:58.329445Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac2937-9ecb-479e-ac3e-faa2e559a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdcdd8e3-123d-4fe2-8281-dc30023808f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T16:24:20.628408Z",
     "iopub.status.busy": "2024-05-29T16:24:20.628336Z",
     "iopub.status.idle": "2024-05-29T16:24:20.641724Z",
     "shell.execute_reply": "2024-05-29T16:24:20.641429Z",
     "shell.execute_reply.started": "2024-05-29T16:24:20.628401Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather = weather.repartition(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eef811e6-cf9d-463c-ae28-68479e01e1bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T16:24:20.642204Z",
     "iopub.status.busy": "2024-05-29T16:24:20.642136Z",
     "iopub.status.idle": "2024-05-29T16:24:20.760938Z",
     "shell.execute_reply": "2024-05-29T16:24:20.760694Z",
     "shell.execute_reply.started": "2024-05-29T16:24:20.642197Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29daf5-66e9-45d1-963a-0be6fe66e8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d59a5132-58e0-4150-936a-4d6758ad411f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T16:24:20.767751Z",
     "iopub.status.busy": "2024-05-29T16:24:20.767696Z",
     "iopub.status.idle": "2024-05-29T16:24:20.769440Z",
     "shell.execute_reply": "2024-05-29T16:24:20.769212Z",
     "shell.execute_reply.started": "2024-05-29T16:24:20.767745Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription('Count the year month day')  # Setting Job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d0e7b7-5f02-4fe9-8330-eec56d66ac49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T16:24:20.769913Z",
     "iopub.status.busy": "2024-05-29T16:24:20.769843Z",
     "iopub.status.idle": "2024-05-29T16:24:20.857212Z",
     "shell.execute_reply": "2024-05-29T16:24:20.856843Z",
     "shell.execute_reply.started": "2024-05-29T16:24:20.769907Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather = weather.groupby(\"year\",\"month\",\"day\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16302b9a-5f04-4dff-8878-836b1ae3783f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T16:24:20.857767Z",
     "iopub.status.busy": "2024-05-29T16:24:20.857682Z",
     "iopub.status.idle": "2024-05-29T16:24:38.533838Z",
     "shell.execute_reply": "2024-05-29T16:24:38.533482Z",
     "shell.execute_reply.started": "2024-05-29T16:24:20.857758Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-------+\n",
      "|year|month|day|  count|\n",
      "+----+-----+---+-------+\n",
      "|2017|    9|  7|1230518|\n",
      "|2017|    9| 30|1230518|\n",
      "|2017|    9| 15|1230518|\n",
      "|2017|    8|  4|1230518|\n",
      "|2017|    9| 25|1230518|\n",
      "|2017|    8|  6|1230518|\n",
      "|2016|   10| 21|1204295|\n",
      "|2016|   10| 14|1204295|\n",
      "|2016|   10|  8|1204295|\n",
      "|2017|    8| 29|1230518|\n",
      "|2017|    9| 21|1230518|\n",
      "|2017|    8| 12|1230518|\n",
      "|2016|   10|  1|1204295|\n",
      "|2017|    9|  9|1230518|\n",
      "|2017|    9|  6|1230518|\n",
      "|2017|    8| 19|1230518|\n",
      "|2016|   10| 12|1204295|\n",
      "|2016|   10| 25|1204295|\n",
      "|2016|   10|  4|1204295|\n",
      "|2016|   10| 22|1204295|\n",
      "+----+-----+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e5c5d30-1016-4b95-9e36-6173b387b177",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T16:24:38.534445Z",
     "iopub.status.busy": "2024-05-29T16:24:38.534366Z",
     "iopub.status.idle": "2024-05-29T16:24:38.760873Z",
     "shell.execute_reply": "2024-05-29T16:24:38.760632Z",
     "shell.execute_reply.started": "2024-05-29T16:24:38.534437Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21c444db-28b2-4f1e-b60b-9de1fec24cc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T16:24:38.761316Z",
     "iopub.status.busy": "2024-05-29T16:24:38.761240Z",
     "iopub.status.idle": "2024-05-29T16:24:38.763155Z",
     "shell.execute_reply": "2024-05-29T16:24:38.762893Z",
     "shell.execute_reply.started": "2024-05-29T16:24:38.761310Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription('do noop')  # Setting Job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a18feec9-6dd2-4c49-9c65-3d7e94ae2b40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T16:24:38.763504Z",
     "iopub.status.busy": "2024-05-29T16:24:38.763444Z",
     "iopub.status.idle": "2024-05-29T16:24:47.494477Z",
     "shell.execute_reply": "2024-05-29T16:24:47.494062Z",
     "shell.execute_reply.started": "2024-05-29T16:24:38.763497Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"SET spark.sql.shuffle.partitions=10\")\n",
    "\n",
    "\n",
    "#spark.read.parquet(parquet_path).write.format(\"noop\").mode(\"overwrite\").save() ## Test with a noop write\n",
    "\n",
    "weather.write.format(\"noop\").mode(\"overwrite\").save() ## Test with a noop write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "054cce92-c500-4518-8589-a8f1260f2684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T16:24:47.495251Z",
     "iopub.status.busy": "2024-05-29T16:24:47.495143Z",
     "iopub.status.idle": "2024-05-29T16:24:47.499323Z",
     "shell.execute_reply": "2024-05-29T16:24:47.499075Z",
     "shell.execute_reply.started": "2024-05-29T16:24:47.495241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bd61d68-7141-46b9-b247-b5f50eb6b74b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T16:24:47.499804Z",
     "iopub.status.busy": "2024-05-29T16:24:47.499741Z",
     "iopub.status.idle": "2024-05-29T16:24:47.503650Z",
     "shell.execute_reply": "2024-05-29T16:24:47.503451Z",
     "shell.execute_reply.started": "2024-05-29T16:24:47.499798Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.57:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10aecbb10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81e073e1-449e-4d4c-9ff8-3724042322b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T16:24:47.504282Z",
     "iopub.status.busy": "2024-05-29T16:24:47.504143Z",
     "iopub.status.idle": "2024-05-29T16:24:47.506694Z",
     "shell.execute_reply": "2024-05-29T16:24:47.506491Z",
     "shell.execute_reply.started": "2024-05-29T16:24:47.504272Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7950466-6466-4321-b3e2-b00bf8c875a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T16:24:47.699008Z",
     "iopub.status.busy": "2024-05-29T16:24:47.698956Z",
     "iopub.status.idle": "2024-05-29T16:24:47.702211Z",
     "shell.execute_reply": "2024-05-29T16:24:47.701906Z",
     "shell.execute_reply.started": "2024-05-29T16:24:47.699003Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daf2c7e8-e47e-4b01-987b-f1b1a4933b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T14:20:15.107886Z",
     "iopub.status.busy": "2024-05-29T14:20:15.107827Z",
     "iopub.status.idle": "2024-05-29T14:20:15.110634Z",
     "shell.execute_reply": "2024-05-29T14:20:15.110376Z",
     "shell.execute_reply.started": "2024-05-29T14:20:15.107880Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-1716992376481\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc = spark.sparkContext\n",
    "app_id = sc.applicationId\n",
    "print(app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ab48be3-7951-483a-a982-160428e78964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:50:55.484570Z",
     "iopub.status.busy": "2024-05-28T20:50:55.484059Z",
     "iopub.status.idle": "2024-05-28T20:50:55.543275Z",
     "shell.execute_reply": "2024-05-28T20:50:55.542811Z",
     "shell.execute_reply.started": "2024-05-28T20:50:55.484541Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "\n",
    "def identify_parallel_stages(app_id):   \n",
    "    base_url = \"http://localhost:4040/api/v1/applications/\" # change to your Spark master node hostname\n",
    "    stages_resp =  requests.get(f\"{base_url}{app_id}/stages/\")\n",
    "    stages = stages_resp.json()\n",
    "\n",
    "    for stage in stages:\n",
    "        if 'submissionTime' in stage and stage['submissionTime'] is not None:\n",
    "            stage['submissionTime'] = datetime.strptime(stage['submissionTime'], '%Y-%m-%dT%H:%M:%S.%fGMT')\n",
    "\n",
    "    stages.sort(key= lambda x: x['submissionTime'] if 'submissionTime' in x and x['submissionTime'] is not None else datetime.min)\n",
    "    \n",
    "    # Identify and print parallel stage info\n",
    "    parallel_groups = []\n",
    "    curr_group = [stages[0]]\n",
    "    for i in range(1, len(stages)):\n",
    "        if stages[i]['submissionTime'] < curr_group[-1]['completionTime']:\n",
    "            curr_group.append(stages[i])\n",
    "        else:\n",
    "            parallel_groups.append(curr_group)\n",
    "            curr_group = [stages[i]]\n",
    "    parallel_groups.append(curr_group)\n",
    "\n",
    "    for i, group in enumerate(parallel_groups):\n",
    "        print(f\"\\nParallel Group {i+1}: {len(group)} stages ran in parallel\")\n",
    "        for stage in group:\n",
    "            print(f\"Stage ID: {stage['stageId']}, Name: {stage['name']}, Submission Time: {stage['submissionTime']}, Completion Time: {stage['completionTime']}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2218130-c5f1-4159-8516-39272745b00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:50:56.141618Z",
     "iopub.status.busy": "2024-05-28T20:50:56.141150Z",
     "iopub.status.idle": "2024-05-28T20:50:56.150970Z",
     "shell.execute_reply": "2024-05-28T20:50:56.150346Z",
     "shell.execute_reply.started": "2024-05-28T20:50:56.141593Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_stage_info(app_id):  \n",
    "    base_url = \"http://localhost:4040/api/v1/applications/\" # change to your Spark master node hostname\n",
    "    stages_resp =  requests.get(f\"{base_url}{app_id}/stages/\")\n",
    "    stages = stages_resp.json()\n",
    "\n",
    "    # Analyze tasks and skew for each stage\n",
    "    for stage in stages:\n",
    "        response_tasks = requests.get(f\"{base_url}{app_id}/stages/{stage['stageId']}/{stage['attemptId']}/taskList\")\n",
    "        tasks = response_tasks.json()\n",
    "\n",
    "        if tasks:\n",
    "            tasks_durations = [task['taskMetrics']['executorRunTime'] for task in tasks]\n",
    "            min_task_duration = min(tasks_durations)\n",
    "            median_task_duration = statistics.median(tasks_durations)\n",
    "            max_task_duration = max(tasks_durations)\n",
    "            task_skew = max_task_duration/median_task_duration if median_task_duration > 0 else 0\n",
    "            task_skew = round(task_skew, 2)\n",
    "        \n",
    "            # Collect additional spill and shuffle metrics\n",
    "            memory_spill = sum(task['taskMetrics']['memoryBytesSpilled'] for task in tasks)\n",
    "            disk_spill = sum(task['taskMetrics']['diskBytesSpilled'] for task in tasks)\n",
    "            shuffle_read = sum(task['taskMetrics']['shuffleReadMetrics']['totalBytesRead'] for task in tasks if 'shuffleReadMetrics' in task['taskMetrics'])\n",
    "            shuffle_write = sum(task['taskMetrics']['shuffleWriteMetrics']['shuffleBytesWritten'] for task in tasks if 'shuffleWriteMetrics' in task['taskMetrics'])\n",
    "\n",
    "            print(f\"\\nAnalysis for Stage ID: {stage['stageId']}, Stage name: {stage['name']}\")\n",
    "            print(f\"Number of tasks: {stage['numTasks']}\")\n",
    "            print(f\"Number of active tasks: {stage['numActiveTasks']}\")\n",
    "            print(f\"Number of completed tasks: {stage['numCompleteTasks']}\")\n",
    "            print(f\"Median task duration: {median_task_duration} ms\")\n",
    "            print(f\"Max task duration: {max_task_duration} ms\")\n",
    "            print(f\"Min task duration: {min_task_duration} ms\")\n",
    "            print(f\"Task Skew: {task_skew}\")  \n",
    "            print(f\"Memory spill: {memory_spill} bytes\")\n",
    "            print(f\"Disk spill: {disk_spill} bytes\")\n",
    "            print(f\"Shuffle read: {shuffle_read} bytes\")\n",
    "            print(f\"Shuffle write: {shuffle_write} bytes\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d5cdcae-cb92-4325-bb09-f0c6062f7a1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:50:57.377007Z",
     "iopub.status.busy": "2024-05-28T20:50:57.376333Z",
     "iopub.status.idle": "2024-05-28T20:50:59.457864Z",
     "shell.execute_reply": "2024-05-28T20:50:59.457383Z",
     "shell.execute_reply.started": "2024-05-28T20:50:57.376962Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'totalBytesRead'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m print_stage_info(app_id)\n",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m, in \u001b[0;36mprint_stage_info\u001b[0;34m(app_id)\u001b[0m\n\u001b[1;32m     20\u001b[0m memory_spill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemoryBytesSpilled\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks)\n\u001b[1;32m     21\u001b[0m disk_spill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiskBytesSpilled\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks)\n\u001b[0;32m---> 22\u001b[0m shuffle_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffleReadMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotalBytesRead\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffleReadMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m shuffle_write \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffleWriteMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffleBytesWritten\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffleWriteMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalysis for Stage ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstageId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Stage name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m memory_spill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemoryBytesSpilled\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks)\n\u001b[1;32m     21\u001b[0m disk_spill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiskBytesSpilled\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks)\n\u001b[0;32m---> 22\u001b[0m shuffle_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffleReadMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotalBytesRead\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffleReadMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m shuffle_write \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffleWriteMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffleBytesWritten\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffleWriteMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaskMetrics\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalysis for Stage ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstageId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Stage name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'totalBytesRead'"
     ]
    }
   ],
   "source": [
    "\n",
    "print_stage_info(app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c52ac0f8-84e1-47d6-898c-6304d702c153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T19:54:59.519333Z",
     "iopub.status.busy": "2024-05-28T19:54:59.517555Z",
     "iopub.status.idle": "2024-05-28T19:54:59.628653Z",
     "shell.execute_reply": "2024-05-28T19:54:59.628163Z",
     "shell.execute_reply.started": "2024-05-28T19:54:59.519230Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'submissionTime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[210], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m identify_parallel_stages(app_id)\n",
      "Cell \u001b[0;32mIn[206], line 20\u001b[0m, in \u001b[0;36midentify_parallel_stages\u001b[0;34m(app_id)\u001b[0m\n\u001b[1;32m     18\u001b[0m curr_group \u001b[38;5;241m=\u001b[39m [stages[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(stages)):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stages[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmissionTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m curr_group[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletionTime\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     21\u001b[0m         curr_group\u001b[38;5;241m.\u001b[39mappend(stages[i])\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'submissionTime'"
     ]
    }
   ],
   "source": [
    "identify_parallel_stages(app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9500bf95-eb9a-4ebc-ab4b-a967133b3f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:51:19.684027Z",
     "iopub.status.busy": "2024-05-28T20:51:19.683329Z",
     "iopub.status.idle": "2024-05-28T20:51:19.772399Z",
     "shell.execute_reply": "2024-05-28T20:51:19.772012Z",
     "shell.execute_reply.started": "2024-05-28T20:51:19.683981Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url = \"http://localhost:4040/api/v1/applications/\" # change to your Spark master node hostname\n",
    "stages_resp =  requests.get(f\"{base_url}{app_id}/stages/\")\n",
    "stages = stages_resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "483f0e98-db96-4bf7-9fee-7cb0011c3632",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:51:35.972557Z",
     "iopub.status.busy": "2024-05-28T20:51:35.971617Z",
     "iopub.status.idle": "2024-05-28T20:51:35.981915Z",
     "shell.execute_reply": "2024-05-28T20:51:35.981264Z",
     "shell.execute_reply.started": "2024-05-28T20:51:35.972500Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'COMPLETE',\n",
       " 'stageId': 0,\n",
       " 'attemptId': 0,\n",
       " 'numTasks': 1,\n",
       " 'numActiveTasks': 0,\n",
       " 'numCompleteTasks': 1,\n",
       " 'numFailedTasks': 0,\n",
       " 'numKilledTasks': 0,\n",
       " 'numCompletedIndices': 1,\n",
       " 'submissionTime': '2024-05-28T20:50:25.690GMT',\n",
       " 'firstTaskLaunchedTime': '2024-05-28T20:50:26.304GMT',\n",
       " 'completionTime': '2024-05-28T20:50:27.172GMT',\n",
       " 'executorDeserializeTime': 515,\n",
       " 'executorDeserializeCpuTime': 449006000,\n",
       " 'executorRunTime': 264,\n",
       " 'executorCpuTime': 96017000,\n",
       " 'resultSize': 1869,\n",
       " 'jvmGcTime': 0,\n",
       " 'resultSerializationTime': 10,\n",
       " 'memoryBytesSpilled': 0,\n",
       " 'diskBytesSpilled': 0,\n",
       " 'peakExecutionMemory': 0,\n",
       " 'inputBytes': 0,\n",
       " 'inputRecords': 0,\n",
       " 'outputBytes': 0,\n",
       " 'outputRecords': 0,\n",
       " 'shuffleRemoteBlocksFetched': 0,\n",
       " 'shuffleLocalBlocksFetched': 0,\n",
       " 'shuffleFetchWaitTime': 0,\n",
       " 'shuffleRemoteBytesRead': 0,\n",
       " 'shuffleRemoteBytesReadToDisk': 0,\n",
       " 'shuffleLocalBytesRead': 0,\n",
       " 'shuffleReadBytes': 0,\n",
       " 'shuffleReadRecords': 0,\n",
       " 'shuffleCorruptMergedBlockChunks': 0,\n",
       " 'shuffleMergedFetchFallbackCount': 0,\n",
       " 'shuffleMergedRemoteBlocksFetched': 0,\n",
       " 'shuffleMergedLocalBlocksFetched': 0,\n",
       " 'shuffleMergedRemoteChunksFetched': 0,\n",
       " 'shuffleMergedLocalChunksFetched': 0,\n",
       " 'shuffleMergedRemoteBytesRead': 0,\n",
       " 'shuffleMergedLocalBytesRead': 0,\n",
       " 'shuffleRemoteReqsDuration': 0,\n",
       " 'shuffleMergedRemoteReqsDuration': 0,\n",
       " 'shuffleWriteBytes': 0,\n",
       " 'shuffleWriteTime': 0,\n",
       " 'shuffleWriteRecords': 0,\n",
       " 'name': 'parquet at NativeMethodAccessorImpl.java:0',\n",
       " 'details': 'org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\njava.lang.reflect.Method.invoke(Method.java:498)\\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\npy4j.Gateway.invoke(Gateway.java:282)\\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\npy4j.commands.CallCommand.execute(CallCommand.java:79)\\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\\njava.lang.Thread.run(Thread.java:750)',\n",
       " 'schedulingPool': 'default',\n",
       " 'rddIds': [1, 0],\n",
       " 'accumulatorUpdates': [],\n",
       " 'killedTasksSummary': {},\n",
       " 'resourceProfileId': 0,\n",
       " 'peakExecutorMetrics': {'JVMHeapMemory': 0,\n",
       "  'JVMOffHeapMemory': 0,\n",
       "  'OnHeapExecutionMemory': 0,\n",
       "  'OffHeapExecutionMemory': 0,\n",
       "  'OnHeapStorageMemory': 0,\n",
       "  'OffHeapStorageMemory': 0,\n",
       "  'OnHeapUnifiedMemory': 0,\n",
       "  'OffHeapUnifiedMemory': 0,\n",
       "  'DirectPoolMemory': 0,\n",
       "  'MappedPoolMemory': 0,\n",
       "  'ProcessTreeJVMVMemory': 0,\n",
       "  'ProcessTreeJVMRSSMemory': 0,\n",
       "  'ProcessTreePythonVMemory': 0,\n",
       "  'ProcessTreePythonRSSMemory': 0,\n",
       "  'ProcessTreeOtherVMemory': 0,\n",
       "  'ProcessTreeOtherRSSMemory': 0,\n",
       "  'MinorGCCount': 0,\n",
       "  'MinorGCTime': 0,\n",
       "  'MajorGCCount': 0,\n",
       "  'MajorGCTime': 0,\n",
       "  'TotalGCTime': 0},\n",
       " 'isShufflePushEnabled': False,\n",
       " 'shuffleMergersCount': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38845fa9-86f9-434a-a64b-c6e38b5fc8e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    # Analyze tasks and skew for each stage\n",
    "    for stage in stages:\n",
    "        response_tasks = requests.get(f\"{base_url}{app_id}/stages/{stage['stageId']}/{stage['attemptId']}/taskList\")\n",
    "        tasks = response_tasks.json()\n",
    "        for task in tasks:\n",
    "            print(task)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de11a7-08dd-4a2f-aab2-a6bcc03a3178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "285a2d6e-0a4e-4b5f-ac55-48e0ffc3c38a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:22:04.818724Z",
     "iopub.status.busy": "2024-05-28T20:22:04.818223Z",
     "iopub.status.idle": "2024-05-28T20:22:04.843521Z",
     "shell.execute_reply": "2024-05-28T20:22:04.843075Z",
     "shell.execute_reply.started": "2024-05-28T20:22:04.818698Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.30:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pySparkSetup</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11153ab90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3f2c16c-0a87-462d-b2d2-41e6e041ab12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T20:41:10.796259Z",
     "iopub.status.busy": "2024-05-28T20:41:10.794682Z",
     "iopub.status.idle": "2024-05-28T20:41:10.817433Z",
     "shell.execute_reply": "2024-05-28T20:41:10.817006Z",
     "shell.execute_reply.started": "2024-05-28T20:41:10.796176Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3324be2e-15d0-4b7f-b26c-ca030d67d4ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T21:39:16.220063Z",
     "iopub.status.busy": "2024-05-28T21:39:16.219584Z",
     "iopub.status.idle": "2024-05-28T21:39:16.466850Z",
     "shell.execute_reply": "2024-05-28T21:39:16.466550Z",
     "shell.execute_reply.started": "2024-05-28T21:39:16.220034Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = \"http://localhost:4040/api/v1/applications/\"\n",
    "stages_resp =  requests.get(f\"{base_url}{app_id}/stages/\")\n",
    "stages = stages_resp.json()\n",
    "\n",
    "tasks_data = []  # Initialize the JSON list\n",
    "# Analyze tasks and skew for each stage\n",
    "for stage in stages:\n",
    "    response_tasks = requests.get(f\"{base_url}{app_id}/stages/{stage['stageId']}/{stage['attemptId']}/taskList\")\n",
    "    tasks = response_tasks.json()\n",
    "    tasks_data.append(tasks)  # Add tasks data in list\n",
    "\n",
    "# Convert tasks_data list to JSON \n",
    "json_data = json.dumps(tasks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e2f814a1-117a-4781-80b6-d2ed11974aac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T21:39:16.478296Z",
     "iopub.status.busy": "2024-05-28T21:39:16.478217Z",
     "iopub.status.idle": "2024-05-28T21:39:16.779008Z",
     "shell.execute_reply": "2024-05-28T21:39:16.778726Z",
     "shell.execute_reply.started": "2024-05-28T21:39:16.478289Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(spark.sparkContext.parallelize([json_data]))  # Use the json method to read the json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "77616838-7347-420c-9f97-1c11519cb8b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T21:39:17.478110Z",
     "iopub.status.busy": "2024-05-28T21:39:17.477407Z",
     "iopub.status.idle": "2024-05-28T21:39:17.816349Z",
     "shell.execute_reply": "2024-05-28T21:39:17.816021Z",
     "shell.execute_reply.started": "2024-05-28T21:39:17.478062Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------+----------+-----------------+----+-----+----------+-----------+--------------+-----------+------+------+------------+-----------+\n",
      "|accumulatorUpdates|attempt|duration|executorId|gettingResultTime|host|index|launchTime|partitionId|schedulerDelay|speculative|status|taskId|taskLocality|taskMetrics|\n",
      "+------------------+-------+--------+----------+-----------------+----+-----+----------+-----------+--------------+-----------+------+------+------------+-----------+\n",
      "|              NULL|   NULL|    NULL|      NULL|             NULL|NULL| NULL|      NULL|       NULL|          NULL|       NULL|  NULL|  NULL|        NULL|       NULL|\n",
      "+------------------+-------+--------+----------+-----------------+----+-----+----------+-----------+--------------+-----------+------+------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c479d273-33ac-44d9-a6c0-7406f209e0b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-28T21:38:30.707330Z",
     "iopub.status.busy": "2024-05-28T21:38:30.706740Z",
     "iopub.status.idle": "2024-05-28T21:38:32.895569Z",
     "shell.execute_reply": "2024-05-28T21:38:32.894855Z",
     "shell.execute_reply.started": "2024-05-28T21:38:30.707293Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Event` cannot be resolved. Did you mean one of the following? [`host`, `index`, `attempt`, `status`, `taskId`].; line 1 pos 0;\n'Filter ('Event = SparkListenerTaskEnd)\n+- LogicalRDD [accumulatorUpdates#186, attempt#187L, duration#188L, executorId#189, gettingResultTime#190L, host#191, index#192L, launchTime#193, partitionId#194L, schedulerDelay#195L, speculative#196, status#197, taskId#198L, taskLocality#199, taskMetrics#200], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mjson(spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize([json_data]))  \u001b[38;5;66;03m# Use the json method to read the json_data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvent=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparkListenerTaskEnd\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStage ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask Info.*\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask Metrics.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/dataframe.py:3327\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3271\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Filters rows using the given condition.\u001b[39;00m\n\u001b[1;32m   3272\u001b[0m \n\u001b[1;32m   3273\u001b[0m \u001b[38;5;124;03m:func:`where` is an alias for :func:`filter`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3324\u001b[0m \u001b[38;5;124;03m+---+-----+\u001b[39;00m\n\u001b[1;32m   3325\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 3327\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   3328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[1;32m   3329\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition\u001b[38;5;241m.\u001b[39m_jc)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Event` cannot be resolved. Did you mean one of the following? [`host`, `index`, `attempt`, `status`, `taskId`].; line 1 pos 0;\n'Filter ('Event = SparkListenerTaskEnd)\n+- LogicalRDD [accumulatorUpdates#186, attempt#187L, duration#188L, executorId#189, gettingResultTime#190L, host#191, index#192L, launchTime#193, partitionId#194L, schedulerDelay#195L, speculative#196, status#197, taskId#198L, taskLocality#199, taskMetrics#200], false\n"
     ]
    }
   ],
   "source": [
    "df2 = df.filter(\"Event='SparkListenerTaskEnd'\").select(\"Stage ID\", \"Task Info.*\", \"Task Metrics.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2e195-f7a5-4d22-a0e7-0b7aab0ed76d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
